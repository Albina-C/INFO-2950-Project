{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Research Question\n",
    "#### Is there a relationship between how happy a country is and the music the people of this country listen to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data Description](#dd)\n",
    "2. [Preregistration Statements](#pre)\n",
    "3. [Data Analysis – Evaluation of Significance](#da1)\n",
    "4. [Data Analysis – Interpretations and Conclusions](#da2)\n",
    "5. [Limitations](#lims)\n",
    "6. [Data Collection and Cleaning](#collection)\n",
    "7. [Exploratory Data Analysis](#exploratory)\n",
    "8. [Training a model to predict happiness scores in 2020](#training)\n",
    "9. [Sources](#sources)\n",
    "10.[Questions for Reviewers](#questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <a name=\"introduction\"></a>\n",
    "Attempting to understand what makes a country \"happy\" is often attributable to freedom, social support, life expectancy, health, among others. However, the music the people of said country listen to is often not one of these factors given its weak relationship and versatility across the world. Although our research question does not attempt to attribute the type of music a country listens to to its happiness index score (it would be difficult to establish given reverse causality), we are more interested in observing what trends are apparent in some of the happiest and less happiest countries with respect to the music they listen to. \n",
    "\n",
    "It is no surprise that music is widely used as coping mechanism for any form of distress by most individuals across the world. Neuroscientists have attributed songs like \"Don't Stop Me Now\" by Queen and \"Dancing Queen\" by Abba as some of the happiest songs. Indeed, Spotify has thousands of playlists with titles like \"Happy Days,\" \"Happy Hits,\" Just Smile Happy Songs,\" \"Happy Songs to Sing in The Car,\" etcetera. \n",
    "\n",
    "#### Some additional questions that we are seeking to answer...\n",
    "- What songs are there (if any) that are popular in all countries regardless of happiness rank?\n",
    "- Are there genres/styles of music that are consistent with \"happy\" countries? With \"sad\" countries? For example, can we expect happy countries to listen to more pop music than sad ones?\n",
    "- What are the characteristics of songs that are popular in happy countries? Fast or slow tempo? Live vocals or autotune?\n",
    "- Can we find more instances of happy songs in sad countries than sad songs in happy countries?\"\n",
    "\n",
    "#### The datasets we will be using:\n",
    "\n",
    "All the datasets were found on Kaggle.\n",
    "<br>\n",
    "1. [\"World Happiness Report 2017\"](https://www.kaggle.com/unsdsn/world-happiness) by Sustainable Development Solutions Network\n",
    "<br>\n",
    "The Sustainable Development Solutions Network ranks 155 countries by their happiness levels, and calculates a \"happiness score\" for each country using six factors: economic production, social support, life expectancy, freedom, absence of corruption, and generosity. This dataset is for the year 2017.\n",
    "2. [\"Spotify's Worldwide Daily Song Ranking\"](https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking) by Kaggle user\n",
    "<br>\n",
    "For each country in 54 countries, this dataset provides the top 200 songs per day in the year of 2017 (January 1, 2017 to January 9, 2018). \n",
    "- *Note: the Kaggle description says 53 countries, but we found 54 countries. Perhaps the description was not updated when the dataset was.*\n",
    "3. [\"Spotify Web API\"](https://developer.spotify.com/documentation/web-api/reference/) \n",
    "<br>\n",
    "This dataset contains contains characteristics of songs on Spotify. Spotify calculates and gives scores for their songs. Some of these values include scores for danceability, beats per minute (bpm), and liveness (the likeliness that the song is a live recording).\n",
    "4. [\"World Happiness Report 2020](https://www.kaggle.com/londeen/world-happiness-report-2020?select=WHR20_DataForFigure2.1.csv) by Michael Londeen (Kaggle User, adapted from Sustainable Development Solutions Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Data Description <a name=\"dd\"></a>\n",
    "The two datasets we used from Kaggle were a Spotify dataset that compiled the daily top songs from January 2017 to January 2018 for each country and a dataset that compiled the happiness scores and ranks by country. \n",
    "\n",
    "**World Happiness Report Dataset**<br>\n",
    "<br>The happiness score dataset’s rows are the countries that were included in the Gallup World Poll; its columns are country, happiness score, happiness rank, GDP per capita, family, life expectancy, freedom, generosity, government corruption, and dystopia residual. Factors such as GDP per capita, freedom, etc. are used to compute a country’s happiness score, and then their rank once their score is viewed in relation to other countries. \n",
    "<br>Much of the data for the world happiness reports comes from the Gallup World Poll. This poll conducts randomized telephone or face-to-face interviews depending on the percentage of a country’s population that has a telephone (cite). The world happiness report is usually published by the Sustainable Development Solutions Network, a branch of the United Nations. The Sustainable Development Solutions Network uploaded the data set for 2015-2019 onto Kaggle. We only use the 2017 dataset. According to the Kaggle dataset, the inspiration for this dataset was to discover the answers to questions like, “What countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness?”, “How did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports?”,  “Did any country experience a significant increase or decrease in happiness?” (cite). Since this dataset is published by the Sustainable Development Solutions Network, we can assume that the United Nations funded the creation of this dataset – or at least the data within it.\n",
    "<br>While Gallup has a very thorough methodology when it comes to carrying out the Gallup World Poll there is still the possibility that people are not honest when they answer the question, or that the poll is not reaching a large/diverse enough group. Additionally, the data that is collected through this poll is affected by the type of person who typically responds to polling questions; for example, people who have very strong opinions about the topic are more likely to agree to be polled than those who have weaker opinions. This may skew the responses to the data. \n",
    "<br>To compile this dataset, the Sustainable Development Solutions Network had to take the data from the world happiness reports (which have separate reports for each year) and append the poll data into the necessary columns for each country. This required the Sustainable Development Solutions Network to pick and choose the data in the actual world happiness report that they believed was important to include in the Kaggle dataset. \n",
    "The happiness scores are calculated using data that is collected through the Gallup World Poll. The people who answer these questions are aware of the data collection. This is a well-known report that is compiled, however, we are unsure if they are aware of the purpose of the survey.\n",
    "This is the link to the dataset that was uploaded to Kaggle by the SDSN: https://www.kaggle.com/unsdsn/world-happiness \n",
    "\n",
    "**Spotify Dataset**<br>\n",
    "<br>Each row of this dataset contains a ranking position on a specific day for a song. The columns represent different features of these songs, including position, track name, artist, streams, URL, date, and region. It should be noted that this dataset contains 200 top songs for a given day for 53 countries, which are represented by abbreviated regions.\n",
    "<br>This dataset was created to identify trends among certain groups of songs within different regions, which would theoretically allow people to predict specific ranks of songs and their respective streams in Spotify. The API, thus, was intended for observing potential  correlations between the data. \n",
    "<br>A Kaggle user whose name is Eduardo created this dataset. He is said to be a Software Engineer based in France for the company BlaBlaCar. It is likely the user may have excluded some songs out of this dataset whose information was not entirely present, or that had some data missing. In particular, the user explains the data was missing on a few occasions, so some countries may not have 200 songs for each day from 2017-2018 due to missing data. Thus, only the songs from Spotify that were available at the time of the web-scraping were added to the dataset. The creator of this dataset web-scraped the Spotify charts and we downloaded the Kaggle CSV File of this dataset in order to obtain the information for the songs. No users besides Eduardo were involved in the process according to the specifications of the dataset.\n",
    "https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III Pre-registration Statements <a name=\"pre\"></a>\n",
    "\n",
    "The two analyses that we have chosen to perform in our final project are:\n",
    "\n",
    "1. What is the relationship between the happiness score of a country and the average danceability score of its top songs over two-week periods for a year?\n",
    "2. What is the relationship between the happiness score of a country and the average valence score of its top songs over two-week periods for a year?\n",
    "<br>\n",
    "**Analysis #1**<br> Does the danceability of a song have any correlation with the happiness score of a country?\n",
    "For our first analysis, we will be performing the rolling average of two weeks (14 days) for our data frame. In sum, this rolling average will take the average danceability score over 14 days by collecting the 50 songs for this period for the top happiest and bottom happiest countries. In particular, we believe there may be a relationship between those countries that score high in the happiness rank and those countries with songs that have high danceability scores. Danceability is often correlated with happiness, hence we believe the top ten [happiest] countries may exhibit particularly high danceability scores. The goal of our project is to understand whether there is any meaningful relationship between happy countries and the music they listen to. As a result, performing a rolling average of the danceability scores of the top happiest and least happy countries will allow us to observe one dimension of happiness from the lenses of danceability. As stated, we expect to see the top happiest countries (e.g., Norway and Iceland) to have high danceability rolling average scores compared to the least happy countries (e.g., Honduras and Portugal).\n",
    "\n",
    "**Analysis #2** <br>Can we predict the happiness ranking of a country based on Spotify-provided happiness scores (“valence”)?\n",
    "For every song, the Spotify API provides its own “happiness” score, called “valence.” The higher the valence score, the more positive mood for the song. Using these provided happiness scores, we want to compare the valence for popular songs in sad countries and popular songs in happy countries. If happy countries do have higher valence scores than sad ones, we may find a relevant connection between happiness ranks and the valence scores. If sad countries have higher valence scores than happy countries, we may still be able to find a connection between the variables and presume that sad countries may want to listen to happier songs. Even if there is no significant difference between the valence scores between happy and sad countries, we can still analyze the results and conduct more outside research as to why there is no correlation. This may be related to other Spotify-provided scores, such as “danceability”. Some countries, perhaps Hispanic ones, may have dance built into their cultures even though the happiness scores are variable. All in all, we may find that Spotify’s valence score does not accurately indicate happiness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Data Analysis <a name=\"da1\"></a>\n",
    "**Evaluation of significance**<br>\n",
    "We used various techniques to analyze our data, but our main analysis consisted of a linear regression model to predict happiness score for 2020 based on data we trained from 2017. Specifically, we focused on using certain features (danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, and duration_ms) as our predictor variables and our happiness score as our outcome variable. Doing a linear regression model was appropriate for our model because we had already done the correlation of some of these features and happiness score, which all resulted in insignificant results. Therefore, modeling the relationship between happiness score and these features seemed most adequate by fitting a linear equation to the observed data. This was also in line with our hypothesis asserting a positive relationship between happiness score and these features (i.e., we expected our top ten countries to have a positive, quasi-linear relationship between happiness score and some of these features, and the opposite with our bottom ten countries).\n",
    "<br><br>\n",
    "It should be noted that we deemed correlation appropriate – and consequently linear regression – after having completed the rolling average of these top and bottom ten countries for a window of 14 days of Spotify songs’ features. We observed virtually no relationship between some of our features like danceability and valence – whose graphs support this conclusion – and happiness score, hence we resorted to other methods of potentially evaluating this relationship. As it pertains to our reasoning behind deciding to run a rolling average, we wanted to understand the possibility of there being a relationship between happiness score and individual features over fourteen days. A rolling, or moving, average analyzes data points by creating a series of averages of different subsets of a full data set, which seemed appropriate for our purposes since we were working with a dataset of ~50 countries for one year. Although we acknowledge that it would have been more compelling to evaluate a rolling average over a longer period [beyond fourteen days], this was impractical given time constraints. We attempted to do this, and the calculation took over three hours; further, the Spotify API – which we are using to obtain all the features for our Spotify songs – could only be run a specific number of times before we got a timing error. \n",
    "<br><br>\n",
    "Our correlation scores indicated there was no strong positive relationship between happiness score and each feature. Our strongest correlation (0.35)  was that between happiness score and duration_ms, which was a trivial feature for our analysis. Nevertheless, we observed slightly more positive correlations when it came to happiness rank (note: this is not the same as happiness score) and liveness (0.7), which prompted us to try the rolling average of individual features and consequently try a linear regression model. Concerning our linear regression training model, our coefficients for all features were insignificant. Nearly all of our coefficients equaled zero, except for some features like \"acousticness\" and \"liveness\", which still produced results <0.03.\n",
    "<br><br><br>\n",
    "**Interpretation and conclusions** <a name=\"da2\"></a>\n",
    "Upon having run a linear regression model to predict happiness score for 2020 by training 2017 data – specifically, some of the Spotify features–  our coefficients (which determines the direction of the relationship between our predictor and outcome) were 0 or close to 0. Further, we conducted a rolling average of two of these Spotify features for songs over two weeks for the top and bottom 10 countries in 2017, and our results indicated that there was no significant relationship between happiness score and these Spotify features. As such, we can only conclude that the selected features for Spotify are not a good predicting tool for a country’s happiness, as determined by its happiness score.\n",
    "<br><br>\n",
    "We predicted a country’s happiness score could be predicted by the type of music the people of that country listen to. Specifically, we thought features like how danceable a song is would ultimately give us more insight into the overall happiness of people from this country – irrespective of social or political factors affecting this score. Following conventional wisdom, we believed features like danceability, valence, instrumentalism, and liveness were representative of happiness. However, our results indicate precisely the opposite, thus making these features unfit for determining a country’s happiness. \n",
    "<br><br>\n",
    "Without prior knowledge about this potential relationship between a country’s happiness and these Spotify features, analysts could potentially predict a country’s general happiness using features like liveness and danceability. This can be seen as a useful metric for this purpose, given the ability of music to uplift or distress people, which would be generalized to the entire population of certain countries. In particular, we often associate music with mood, given the ability of music to stimulate certain emotions or sensations. A prediction of this sort would theoretically impulse policy-makers – or anyone striving for the overall well-being of a given country – to implement measures and activities using music. Countries ranking low on their happiness score would attempt to assimilate to the music followings of higher-ranking countries on their happiness score. As it relates, pursuing a goal like this is not indicative of a country’s happiness and will likely produce a negligible effect on the country’s happiness. \n",
    "<br><br>\n",
    "Notwithstanding, there may be positive outcomes in following through with this approach. Although this would require further research on the subject matter, the exposure to “happy” music – as determined by high scores of danceability, or other similar features – can improve the well-being and emotional state of people.  Indeed, authors writing on behalf of the American Psychological Association studied precisely this relationship in 2010, asserting that “happiness ratings were elevated for fast-tempo and major key-stimuli and sadness ratings were elevated for slow-tempo and minor-key stimuli'' (Hunter). A different relationship may be observed in a longer period than 14 weeks, but this would require the use of more developed tools beyond only using the Spotify API, given the constraints it poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Data Limitations <a name=\"lims\"></a>\n",
    "When conducting this analysis, our group used only the top ten and bottom ten happiest countries, due to the limit on the number of calls you can make to the Spotify API. This limits our data size because we excluded the rest of the countries that are included within the happiness data. This affects our data by not only limiting it but also excluding countries with different cultures and the trends within that nation. \n",
    "<br><br>\n",
    "Using data from 2017 further limits our data set because it is not up-to-date. We recognize that within our analysis that the cultures of the countries that we selected are not varying significantly, so we assumed that the type of music listened to within the different countries would not differ greatly. \n",
    "<br><br>\n",
    "\n",
    "Using the music platform Spotify in itself for this analysis is another limitation because it excludes the other music platforms that may be more popular in different countries. Japan serves as a prime example of this limitation coming into play because Japan does not use Spotify as much as other countries. \n",
    "<br><br>\n",
    "\n",
    "While attempting to extract information from the Spotify API, our group came across issues with the security of Spotify. When re-running our code, there would be errors yielded which stated that we used our maximum amounts of retries. To mitigate this issue, we had to install a Spotify extension for our use. Additionally, Spotify installed an extra layer of security on their Spotify charts website, which hindered our ability to web-scrape the data for 2020. We were able to get 2020 data, however, some days may be missing for certain countries.  \n",
    "<br>\n",
    "Another limitation our group recognized was that music listened to globally became more versatile in that people of different nationalities are listening to international music, due to trends. This change might not be reflected within our data analysis because, once again, we are working with data from 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI Data Collection and Cleaning <a name=\"collection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant packages\n",
    "import requests #package for http requests\n",
    "import bs4 # package for html parsing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#Spotify API\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "\n",
    "\n",
    "#cid AND secret IS SPECIFIC TO EACH USER. SO EVA, ESTELLE, AND CESAR SHOULD HAVE DIFFERENT CID AND SECRET. \n",
    "# Do we have to each make our own cid and secret? or can we just use mine?\n",
    "#Eva's:\n",
    "cid = '24ac9ca75f06477ca560d8c71807dd9e'\n",
    "secret = '086fa8a3b056408e9bce55e3245c4af1'\n",
    "\n",
    "# cid='5decb5b36c3f4465a4aafb6bdf035e5d'\n",
    "# secret='dcc15f6bb7254c12bede09644e0fb24b'\n",
    "\n",
    "#Estelle's:\n",
    "# cid='c10b42de14134edfb7e9cafa42fc48a2'\n",
    "# secret='b41981d56a924e65a079138f9272e8de'\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager\n",
    "                    =client_credentials_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data cleaning for \"World Happiness Report\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>happiness_rank</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>whisker_high</th>\n",
       "      <th>whisker_low</th>\n",
       "      <th>economy_gdp_per_capita_</th>\n",
       "      <th>family</th>\n",
       "      <th>health_life_expectancy_</th>\n",
       "      <th>freedom</th>\n",
       "      <th>generosity</th>\n",
       "      <th>trust_government_corruption_</th>\n",
       "      <th>dystopia_residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norway</td>\n",
       "      <td>1</td>\n",
       "      <td>7.537</td>\n",
       "      <td>7.594445</td>\n",
       "      <td>7.479556</td>\n",
       "      <td>1.616463</td>\n",
       "      <td>1.533524</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.635423</td>\n",
       "      <td>0.362012</td>\n",
       "      <td>0.315964</td>\n",
       "      <td>2.277027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>2</td>\n",
       "      <td>7.522</td>\n",
       "      <td>7.581728</td>\n",
       "      <td>7.462272</td>\n",
       "      <td>1.482383</td>\n",
       "      <td>1.551122</td>\n",
       "      <td>0.792566</td>\n",
       "      <td>0.626007</td>\n",
       "      <td>0.355280</td>\n",
       "      <td>0.400770</td>\n",
       "      <td>2.313707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>3</td>\n",
       "      <td>7.504</td>\n",
       "      <td>7.622030</td>\n",
       "      <td>7.385970</td>\n",
       "      <td>1.480633</td>\n",
       "      <td>1.610574</td>\n",
       "      <td>0.833552</td>\n",
       "      <td>0.627163</td>\n",
       "      <td>0.475540</td>\n",
       "      <td>0.153527</td>\n",
       "      <td>2.322715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>4</td>\n",
       "      <td>7.494</td>\n",
       "      <td>7.561772</td>\n",
       "      <td>7.426227</td>\n",
       "      <td>1.564980</td>\n",
       "      <td>1.516912</td>\n",
       "      <td>0.858131</td>\n",
       "      <td>0.620071</td>\n",
       "      <td>0.290549</td>\n",
       "      <td>0.367007</td>\n",
       "      <td>2.276716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finland</td>\n",
       "      <td>5</td>\n",
       "      <td>7.469</td>\n",
       "      <td>7.527542</td>\n",
       "      <td>7.410458</td>\n",
       "      <td>1.443572</td>\n",
       "      <td>1.540247</td>\n",
       "      <td>0.809158</td>\n",
       "      <td>0.617951</td>\n",
       "      <td>0.245483</td>\n",
       "      <td>0.382612</td>\n",
       "      <td>2.430182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  happiness_rank  happiness_score  whisker_high  whisker_low  \\\n",
       "0       Norway               1            7.537      7.594445     7.479556   \n",
       "1      Denmark               2            7.522      7.581728     7.462272   \n",
       "2      Iceland               3            7.504      7.622030     7.385970   \n",
       "3  Switzerland               4            7.494      7.561772     7.426227   \n",
       "4      Finland               5            7.469      7.527542     7.410458   \n",
       "\n",
       "   economy_gdp_per_capita_    family  health_life_expectancy_   freedom  \\\n",
       "0                 1.616463  1.533524                 0.796667  0.635423   \n",
       "1                 1.482383  1.551122                 0.792566  0.626007   \n",
       "2                 1.480633  1.610574                 0.833552  0.627163   \n",
       "3                 1.564980  1.516912                 0.858131  0.620071   \n",
       "4                 1.443572  1.540247                 0.809158  0.617951   \n",
       "\n",
       "   generosity  trust_government_corruption_  dystopia_residual  \n",
       "0    0.362012                      0.315964           2.277027  \n",
       "1    0.355280                      0.400770           2.313707  \n",
       "2    0.475540                      0.153527           2.322715  \n",
       "3    0.290549                      0.367007           2.276716  \n",
       "4    0.245483                      0.382612           2.430182  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"World Happiness Report 2017\"\n",
    "happy2017=pd.read_csv(\"2017.csv\")\n",
    "#cleaning up col names\n",
    "happy2017columns= happy2017.columns\n",
    "happy2017columns= [x.lower() for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\"..\",\".\") for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\".\",\"_\") for x in happy2017columns]\n",
    "happy2017.columns= happy2017columns\n",
    "happy2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>happiness_rank</th>\n",
       "      <th>happiness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norway</td>\n",
       "      <td>1</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>2</td>\n",
       "      <td>7.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>3</td>\n",
       "      <td>7.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>4</td>\n",
       "      <td>7.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finland</td>\n",
       "      <td>5</td>\n",
       "      <td>7.469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  happiness_rank  happiness_score\n",
       "0       Norway               1            7.537\n",
       "1      Denmark               2            7.522\n",
       "2      Iceland               3            7.504\n",
       "3  Switzerland               4            7.494\n",
       "4      Finland               5            7.469"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing all columns except for \"country\", \"happiness_rank\", and \"happiness_score\"\n",
    "happy2017=happy2017.iloc[:,:3]\n",
    "happy2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\"Spotify's Worldwide Song Ranking\"\n",
    "#This .csv file was so big that not only could we not push it to GitHub, but it was also difficult to load the file on Sheets.\n",
    "#Locally on her own computer, Eva split up data.csv into 53 individual .csv files by country so that we work with the data.\n",
    "\n",
    "#allspotifydata=pd.read_csv(\"data.csv\")\n",
    "#countries=pd.unique(allspotifydata['Region'])\n",
    "\n",
    "#allcountries=[]\n",
    "#for country in countries:\n",
    "    #allcountries.append(allspotifydata[allspotifydata['Region']==country])\n",
    "\n",
    "#count=1\n",
    "#for df in allcountries:\n",
    "    #name='country'+str(count)+'.csv'\n",
    "    #df.to_csv(r'C:\\Users\\Eva\\Downloads\\country'+str(count)+'.csv')\n",
    "    #count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the 54 country .csv split from data.csv\n",
    "argentina=pd.read_csv(\"argentina.csv\")\n",
    "australia=pd.read_csv(\"australia.csv\")\n",
    "austria=pd.read_csv(\"austria.csv\")\n",
    "belgium=pd.read_csv(\"belgium.csv\")\n",
    "bolivia=pd.read_csv(\"bolivia.csv\")\n",
    "brazil=pd.read_csv(\"brazil.csv\")\n",
    "canada=pd.read_csv(\"canada.csv\")\n",
    "chile=pd.read_csv(\"chile.csv\")\n",
    "colombia=pd.read_csv(\"colombia.csv\")\n",
    "costarica=pd.read_csv(\"costarica.csv\")\n",
    "czechrepublic=pd.read_csv(\"czechrepublic.csv\")\n",
    "denmark=pd.read_csv(\"denmark.csv\")\n",
    "dominicanrepublic=pd.read_csv(\"dominicanrepublic.csv\")\n",
    "ecuador=pd.read_csv(\"ecuador.csv\")\n",
    "elsalvador=pd.read_csv('elsalvador.csv')\n",
    "estonia=pd.read_csv('estonia.csv')\n",
    "finland=pd.read_csv(\"finland.csv\")\n",
    "france=pd.read_csv(\"france.csv\")\n",
    "germany=pd.read_csv(\"germany.csv\")\n",
    "Global=pd.read_csv('global.csv')\n",
    "greece=pd.read_csv('greece.csv')\n",
    "guatemala=pd.read_csv(\"guatemala.csv\")\n",
    "honduras=pd.read_csv(\"honduras.csv\")\n",
    "hongkong=pd.read_csv(\"hongkong.csv\")\n",
    "hungary=pd.read_csv(\"hungary.csv\")\n",
    "iceland=pd.read_csv(\"iceland.csv\")\n",
    "indonesia=pd.read_csv(\"indonesia.csv\")\n",
    "ireland=pd.read_csv(\"ireland.csv\")\n",
    "italy=pd.read_csv(\"italy.csv\")\n",
    "japan=pd.read_csv(\"japan.csv\")\n",
    "latvia=pd.read_csv(\"latvia.csv\")\n",
    "lithuania=pd.read_csv(\"lithuania.csv\")\n",
    "luxembourg=pd.read_csv(\"luxembourg.csv\")\n",
    "malaysia=pd.read_csv(\"malaysia.csv\")\n",
    "mexico=pd.read_csv(\"mexico.csv\")\n",
    "netherlands=pd.read_csv(\"netherlands.csv\")\n",
    "newzealand=pd.read_csv(\"newzealand.csv\")\n",
    "norway=pd.read_csv(\"norway.csv\")\n",
    "panama=pd.read_csv(\"panama.csv\")\n",
    "paraguay=pd.read_csv(\"paraguay.csv\")\n",
    "peru=pd.read_csv(\"peru.csv\")\n",
    "philippines=pd.read_csv(\"philippines.csv\")\n",
    "poland=pd.read_csv(\"poland.csv\")\n",
    "portugal=pd.read_csv(\"portugal.csv\")\n",
    "singapore=pd.read_csv(\"singapore.csv\")\n",
    "slovakia=pd.read_csv(\"slovakia.csv\")\n",
    "spain=pd.read_csv(\"spain.csv\")\n",
    "sweden=pd.read_csv(\"sweden.csv\")\n",
    "switzerland=pd.read_csv(\"switzerland.csv\")\n",
    "taiwanprovinceofchina=pd.read_csv(\"taiwan.csv\")\n",
    "turkey=pd.read_csv(\"turkey.csv\")\n",
    "unitedkingdom=pd.read_csv(\"unitedkingdom.csv\")\n",
    "unitedstates=pd.read_csv(\"unitedstates.csv\")\n",
    "uruguay=pd.read_csv(\"uruguay.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creating a new happiness dataframe that contains only the relevant countries\n",
    "As noted in our dataset descriptions, the \"Worldwide Happiness Ranking\\\" (happy17) contains happiness data for 155 countries, while \"Spotify's Worldwide Song Ranking\" contains only 54 countries. <br><br>We needed to find the overlapping countries between these datasets to:\n",
    "1. Create a new happiness ranking excluding the countries not found in the song ranking dataset, subsetted in the dataframe **happy**\n",
    "    <br>\n",
    "2. Figure out which \"Spotify's Worldwide Song Rankings\" country .csv files we do not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 52 countries total that we can use to address our research question.\n",
      "\n",
      "\n",
      "These are the spotify datsets we should use: ['norway', 'denmark', 'iceland', 'switzerland', 'finland', 'netherlands', 'canada', 'newzealand', 'sweden', 'australia', 'costarica', 'austria', 'unitedstates', 'ireland', 'germany', 'belgium', 'luxembourg', 'unitedkingdom', 'chile', 'brazil', 'czechrepublic', 'argentina', 'mexico', 'singapore', 'uruguay', 'guatemala', 'panama', 'france', 'taiwanprovinceofchina', 'spain', 'colombia', 'slovakia', 'malaysia', 'ecuador', 'elsalvador', 'poland', 'italy', 'japan', 'lithuania', 'latvia', 'bolivia', 'peru', 'estonia', 'turkey', 'paraguay', 'philippines', 'hungary', 'indonesia', 'dominicanrepublic', 'greece', 'portugal', 'honduras']\n",
      "\n",
      "\n",
      "This is the updated happiness ranking:\n",
      "                     country  happiness_rank  happiness_score\n",
      "0                     Norway             1.0            7.537\n",
      "1                    Denmark             2.0            7.522\n",
      "2                    Iceland             3.0            7.504\n",
      "3                Switzerland             4.0            7.494\n",
      "4                    Finland             5.0            7.469\n",
      "5                Netherlands             6.0            7.377\n",
      "6                     Canada             7.0            7.316\n",
      "7                New Zealand             8.0            7.314\n",
      "8                     Sweden             9.0            7.284\n",
      "9                  Australia            10.0            7.284\n",
      "10                Costa Rica            12.0            7.079\n",
      "11                   Austria            13.0            7.006\n",
      "12             United States            14.0            6.993\n",
      "13                   Ireland            15.0            6.977\n",
      "14                   Germany            16.0            6.951\n",
      "15                   Belgium            17.0            6.891\n",
      "16                Luxembourg            18.0            6.863\n",
      "17            United Kingdom            19.0            6.714\n",
      "18                     Chile            20.0            6.652\n",
      "19                    Brazil            22.0            6.635\n",
      "20            Czech Republic            23.0            6.609\n",
      "21                 Argentina            24.0            6.599\n",
      "22                    Mexico            25.0            6.578\n",
      "23                 Singapore            26.0            6.572\n",
      "24                   Uruguay            28.0            6.454\n",
      "25                 Guatemala            29.0            6.454\n",
      "26                    Panama            30.0            6.452\n",
      "27                    France            31.0            6.442\n",
      "28  Taiwan Province of China            33.0            6.422\n",
      "29                     Spain            34.0            6.403\n",
      "30                  Colombia            36.0            6.357\n",
      "31                  Slovakia            40.0            6.098\n",
      "32                  Malaysia            42.0            6.084\n",
      "33                   Ecuador            44.0            6.008\n",
      "34               El Salvador            45.0            6.003\n",
      "35                    Poland            46.0            5.973\n",
      "36                     Italy            48.0            5.964\n",
      "37                     Japan            51.0            5.920\n",
      "38                 Lithuania            52.0            5.902\n",
      "39                    Latvia            54.0            5.850\n",
      "40                   Bolivia            58.0            5.823\n",
      "41                      Peru            63.0            5.715\n",
      "42                   Estonia            66.0            5.611\n",
      "43                    Turkey            69.0            5.500\n",
      "44                  Paraguay            70.0            5.493\n",
      "45               Philippines            72.0            5.430\n",
      "46                   Hungary            75.0            5.324\n",
      "47                 Indonesia            81.0            5.262\n",
      "48        Dominican Republic            86.0            5.230\n",
      "49                    Greece            87.0            5.227\n",
      "50                  Portugal            89.0            5.195\n",
      "51                  Honduras            91.0            5.181\n"
     ]
    }
   ],
   "source": [
    "#list of all 54 countries from \"Spotify's Worldwide Songs\"\n",
    "allspotifycountries=[\"argentina\", \"australia\", \"austria\", \"belgium\", \"brazil\",\"bolivia\", \"canada\", \"chile\", \"colombia\", \"costarica\", \"czechrepublic\",\"denmark\", \"dominicanrepublic\", \"estonia\", \"elsalvador\", \"Global\",\"greece\", \"ecuador\", \"finland\", \"france\", \"germany\", \"guatemala\", \"honduras\", \"hongkong\", \"hungary\", \"iceland\", \"indonesia\", \"ireland\", \"italy\", \"japan\", \"latvia\", \"lithuania\", \"luxembourg\", \"malaysia\", \"mexico\", \"netherlands\", \"newzealand\", \"norway\", \"panama\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\", \"singapore\", \"slovakia\", \"spain\", \"sweden\", \"switzerland\", \"taiwanprovinceofchina\", \"turkey\", \"unitedkingdom\", \"unitedstates\", \"uruguay\"]\n",
    "\n",
    "#list for countries that are found in both datsets.\n",
    "allcountries=[]\n",
    "for row in range(len(happy2017)):\n",
    "    country=happy2017.loc[row,'country']\n",
    "    country=country.lower()\n",
    "    country=country.replace(\" \",\"\")\n",
    "    if country in allspotifycountries:\n",
    "        allcountries.append(country)      \n",
    "\n",
    "#new happiness ranking dataframe \"happy\"        \n",
    "happy=pd.DataFrame({'country':[],'happiness_rank':[],'happiness_score':[]})\n",
    "for row in range(len(happy2017)):\n",
    "    country=happy2017.loc[row,'country']\n",
    "    country=country.lower()\n",
    "    country=country.replace(\" \",\"\")\n",
    "    if country in allcountries:       \n",
    "        newrow={'country':happy2017.loc[row,'country'],'happiness_rank':happy2017.loc[row,'happiness_rank'],'happiness_score':happy2017.loc[row,'happiness_score']}\n",
    "        happy=happy.append(newrow, ignore_index=True)\n",
    "\n",
    "print(\"There are \"+ str(len(allcountries)) +\" countries total that we can use to address our research question.\")\n",
    "print(\"\\n\")\n",
    "print('These are the spotify datsets we should use: ' + str(allcountries))\n",
    "print(\"\\n\")\n",
    "print(\"This is the updated happiness ranking:\")\n",
    "print(happy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data cleaning for \"Spotify's Worldwide Song Rankings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the countries we will work with based on the subset below in allcountries. We will use this list to run a for loop to clean the data for each country df\n",
    "countries_to_subset = [norway, denmark, iceland, switzerland, finland, \n",
    "                       netherlands, canada, newzealand, sweden, australia, costarica, austria, \n",
    "                       unitedstates, ireland, germany, belgium, luxembourg, unitedkingdom, \n",
    "                       chile, brazil, czechrepublic, argentina, mexico, singapore, uruguay, guatemala,\n",
    "                       panama, france, taiwanprovinceofchina, spain, colombia, slovakia, malaysia, ecuador, elsalvador, poland, \n",
    "                       italy, japan,lithuania, latvia, bolivia, peru, estonia, turkey, paraguay, \n",
    "                       philippines, hungary, indonesia, dominicanrepublic, greece, portugal, honduras]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Deleting indices that carried over from data.csv\n",
    "As seen in `norway` below, the first column for every country .csv contains an unnamed column. This column, `Unnamed: 0` contains the original indices from the data.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296800</td>\n",
       "      <td>1</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>103886</td>\n",
       "      <td>https://open.spotify.com/track/0JiVRyTJcJnmlwC...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>296801</td>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>85990</td>\n",
       "      <td>https://open.spotify.com/track/5knuzwU65gJK7IF...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296802</td>\n",
       "      <td>3</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>68706</td>\n",
       "      <td>https://open.spotify.com/track/3NdDpSvN911VPGi...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296803</td>\n",
       "      <td>4</td>\n",
       "      <td>Call On Me - Ryan Riback Extended Remix</td>\n",
       "      <td>Starley</td>\n",
       "      <td>60334</td>\n",
       "      <td>https://open.spotify.com/track/78rIJddV4X0HkNA...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>296804</td>\n",
       "      <td>5</td>\n",
       "      <td>I Feel It Coming</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>56607</td>\n",
       "      <td>https://open.spotify.com/track/5GXAXm5YOmYT0kL...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Position                                         Track Name  \\\n",
       "0      296800         1                                              Alone   \n",
       "1      296801         2            Rockabye (feat. Sean Paul & Anne-Marie)   \n",
       "2      296802         3  I Don’t Wanna Live Forever (Fifty Shades Darke...   \n",
       "3      296803         4            Call On Me - Ryan Riback Extended Remix   \n",
       "4      296804         5                                   I Feel It Coming   \n",
       "\n",
       "         Artist  Streams                                                URL  \\\n",
       "0   Alan Walker   103886  https://open.spotify.com/track/0JiVRyTJcJnmlwC...   \n",
       "1  Clean Bandit    85990  https://open.spotify.com/track/5knuzwU65gJK7IF...   \n",
       "2          ZAYN    68706  https://open.spotify.com/track/3NdDpSvN911VPGi...   \n",
       "3       Starley    60334  https://open.spotify.com/track/78rIJddV4X0HkNA...   \n",
       "4    The Weeknd    56607  https://open.spotify.com/track/5GXAXm5YOmYT0kL...   \n",
       "\n",
       "         Date Region  \n",
       "0  2017-01-01     no  \n",
       "1  2017-01-01     no  \n",
       "2  2017-01-01     no  \n",
       "3  2017-01-01     no  \n",
       "4  2017-01-01     no  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norway.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleancol(df):\n",
    "    \"\"\"\n",
    "    This will change the column names to lowercase and replace spaces with underscores.\n",
    "    \n",
    "    Parameter dataframe: this is the country's dataframe which we will work with.\n",
    "    Precondition: a pandas dataframe object\n",
    "    Returns: the dataframe with the cleaned columns.\n",
    "    \"\"\"\n",
    "    new_colnames = df.columns\n",
    "    new_colnames = [x.lower().replace(' ', '_') for x in new_colnames]\n",
    "    df.columns=new_colnames\n",
    "    df=df.drop(columns=['unnamed:_0'])\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. We don't need the top 200 songs per day in a year\n",
    "The \"Spotify Worldwide Song Rankings\" from Kaggle is far too excessive for our analysis and research purposes. Providing the top 200 songs per day in a year means that each country .csv file should have around (365+9)x200=74,800 entries (the additional 9 days are because the data set includes the first 9 days of 2018). We decided that we would remove the bottom 150 songs per day in each country's dataset. <br>\n",
    "It is possible that we may decide to remove even more even later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the countries we will work with based on the subset below in allcountries. We will use this to subset the first 50 songs below\n",
    "countries_to_subset = [norway, denmark, iceland, switzerland, finland, \n",
    "                       netherlands, canada, newzealand, sweden, australia, costarica, austria, \n",
    "                       unitedstates, ireland, germany, belgium, luxembourg, unitedkingdom, \n",
    "                       chile, brazil, czechrepublic, argentina, mexico, singapore, uruguay, guatemala,\n",
    "                       panama, france, taiwanprovinceofchina, spain, colombia, slovakia, malaysia, ecuador, elsalvador, poland, \n",
    "                       italy, japan,lithuania, latvia, bolivia, peru, estonia, turkey, paraguay, \n",
    "                       philippines, hungary, indonesia, dominicanrepublic, greece, portugal, honduras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_fifty(dataframe):\n",
    "    \"\"\"\n",
    "    This will get the first 50 observations in every 200 observations.\n",
    "    There should only be 200 observations in 1 day, and there are 365 days per country in this data,\n",
    "    which is the purpose of this function.\n",
    "    \n",
    "    Parameter dataframe: this is the country's dataframe which we will work with.\n",
    "    Precondition: a pandas dataframe object\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.groupby(\"Date\").head(50)\n",
    "    dataframe=dataframe.reset_index(drop=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now deleting the bottom 150 songs per day in each country...\n",
      "norway length before: 74200\n",
      "norway length after: 74200\n",
      "denmark length before: 74200\n",
      "denmark length after: 74200\n",
      "iceland length before: 35835\n",
      "iceland length after: 35835\n",
      "switzerland length before: 74200\n",
      "switzerland length after: 74200\n",
      "finland length before: 74200\n",
      "finland length after: 74200\n",
      "netherlands length before: 74200\n",
      "netherlands length after: 74200\n",
      "canada length before: 74200\n",
      "canada length after: 74200\n",
      "newzealand length before: 74200\n",
      "newzealand length after: 74200\n",
      "sweden length before: 74200\n",
      "sweden length after: 74200\n",
      "australia length before: 74200\n",
      "australia length after: 74200\n",
      "costarica length before: 74200\n",
      "costarica length after: 74200\n",
      "austria length before: 74200\n",
      "austria length after: 74200\n",
      "unitedstates length before: 74200\n",
      "unitedstates length after: 74200\n",
      "ireland length before: 74200\n",
      "ireland length after: 74200\n",
      "germany length before: 74200\n",
      "germany length after: 74200\n",
      "belgium length before: 74200\n",
      "belgium length after: 74200\n",
      "luxembourg length before: 4098\n",
      "luxembourg length after: 4098\n",
      "unitedkingdom length before: 74200\n",
      "unitedkingdom length after: 74200\n",
      "chile length before: 74200\n",
      "chile length after: 74200\n",
      "brazil length before: 74200\n",
      "brazil length after: 74200\n",
      "czechrepublic length before: 73091\n",
      "czechrepublic length after: 73091\n",
      "argentina length before: 74200\n",
      "argentina length after: 74200\n",
      "mexico length before: 74200\n",
      "mexico length after: 74200\n",
      "singapore length before: 74200\n",
      "singapore length after: 74200\n",
      "uruguay length before: 67816\n",
      "uruguay length after: 67816\n",
      "guatemala length before: 68654\n",
      "guatemala length after: 68654\n",
      "panama length before: 52498\n",
      "panama length after: 52498\n",
      "france length before: 74200\n",
      "france length after: 74200\n",
      "taiwanprovinceofchina length before: 74200\n",
      "taiwanprovinceofchina length after: 74200\n",
      "spain length before: 74200\n",
      "spain length after: 74200\n",
      "colombia length before: 74200\n",
      "colombia length after: 74200\n",
      "slovakia length before: 22597\n",
      "slovakia length after: 22597\n",
      "malaysia length before: 74000\n",
      "malaysia length after: 74000\n",
      "ecuador length before: 74200\n",
      "ecuador length after: 74200\n",
      "elsalvador length before: 38893\n",
      "elsalvador length after: 38893\n",
      "poland length before: 74200\n",
      "poland length after: 74200\n",
      "italy length before: 74200\n",
      "italy length after: 74200\n",
      "japan length before: 72599\n",
      "japan length after: 72599\n",
      "lithuania length before: 16799\n",
      "lithuania length after: 16799\n",
      "latvia length before: 19365\n",
      "latvia length after: 19365\n",
      "bolivia length before: 39838\n",
      "bolivia length after: 39838\n",
      "peru length before: 74200\n",
      "peru length after: 74200\n",
      "estonia length before: 12823\n",
      "estonia length after: 12823\n",
      "turkey length before: 74200\n",
      "turkey length after: 74200\n",
      "paraguay length before: 55797\n",
      "paraguay length after: 55797\n",
      "philippines length before: 74200\n",
      "philippines length after: 74200\n",
      "hungary length before: 60172\n",
      "hungary length after: 60172\n",
      "indonesia length before: 74200\n",
      "indonesia length after: 74200\n",
      "dominicanrepublic length before: 60748\n",
      "dominicanrepublic length after: 60748\n",
      "greece length before: 27192\n",
      "greece length after: 27192\n",
      "portugal length before: 74200\n",
      "portugal length after: 74200\n",
      "honduras length before: 41782\n",
      "honduras length after: 41782\n",
      "\n",
      "\n",
      "Unfortunately the lengths are the same before and after. So we'll have to call first_fifty individally for now.\n"
     ]
    }
   ],
   "source": [
    "# FAILED CODE!!! Please see VI. QUESTIONS FOR REVIEWERS\n",
    "# Our for-loop did not work, as shown in our print statements\n",
    "\n",
    "#running each country in countries_to_subset through the first_fifty procedure\n",
    "print(\"Now deleting the bottom 150 songs per day in each country...\")\n",
    "x=0\n",
    "for file in countries_to_subset:\n",
    "    print(allcountries[x]+\" length before: \"+str(len(countries_to_subset[x])))\n",
    "#   first_fifty(file) #Did not work\n",
    "    file=first_fifty(file) #Did not work\n",
    "#   countries_to_subset[x]=first_fifty(file) #Did not work\n",
    "    print(allcountries[x]+\" length after: \"+str(len(countries_to_subset[x])))\n",
    "    x=x+1\n",
    "print(\"\\n\")\n",
    "print(\"Unfortunately the lengths are the same before and after. So we'll have to call first_fifty individally for now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. We cannot use the data from 2018\n",
    "We only have happiness rankings for the year of 2017, but the Spotify rankings start in January 1, 2017 and stop at January 9, 2018. Though this is only 9 days in 2018, we cannot use this part of the data set.\n",
    "<br>\n",
    "To maintain consistency in out datasets, the function below is excludes all observations, or songs, from 2018 accidentally subsetted in our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not18(dataframe):\n",
    "    dataframe['Date'] = pd.to_datetime(dataframe['Date']).copy()\n",
    "    dataframe = dataframe[dataframe['Date'].dt.year != 2018]\n",
    "    return dataframe\n",
    "#for each in country list, store the return in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'happiness_rank',\n",
       " 'happiness_score',\n",
       " 'whisker_high',\n",
       " 'whisker_low',\n",
       " 'economy_gdp_per_capita_',\n",
       " 'family',\n",
       " 'health_life_expectancy_',\n",
       " 'freedom',\n",
       " 'generosity',\n",
       " 'trust_government_corruption_',\n",
       " 'dystopia_residual']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy2017columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Descriptions\n",
    "- `happiness_score` the sum of each `happiness_score`, `economy_gdp_per_capita_`, `health_life_expectancy_`, `freedom`, `generosity`, `\ttrust_government_corruption_`, and `dystopia_residual` scores. These individual scores reflect the \"six factors\" used to calculate happiness in the description above.<br>\n",
    "- `Happiness.Rank` the ranking of each country's happiness scores, from highest happiness score to the lowest<br>\n",
    "- `Country` the country being ranked/scored<br>\n",
    "\n",
    "* **For our research purposes, we will only be keeping the following columns: `country`,`happiness_rank`, and `happiness_score`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. \"Spotify's World Song Rankings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296800</td>\n",
       "      <td>1</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>103886</td>\n",
       "      <td>https://open.spotify.com/track/0JiVRyTJcJnmlwC...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>296801</td>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>85990</td>\n",
       "      <td>https://open.spotify.com/track/5knuzwU65gJK7IF...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296802</td>\n",
       "      <td>3</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>68706</td>\n",
       "      <td>https://open.spotify.com/track/3NdDpSvN911VPGi...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296803</td>\n",
       "      <td>4</td>\n",
       "      <td>Call On Me - Ryan Riback Extended Remix</td>\n",
       "      <td>Starley</td>\n",
       "      <td>60334</td>\n",
       "      <td>https://open.spotify.com/track/78rIJddV4X0HkNA...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>296804</td>\n",
       "      <td>5</td>\n",
       "      <td>I Feel It Coming</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>56607</td>\n",
       "      <td>https://open.spotify.com/track/5GXAXm5YOmYT0kL...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Position                                         Track Name  \\\n",
       "0      296800         1                                              Alone   \n",
       "1      296801         2            Rockabye (feat. Sean Paul & Anne-Marie)   \n",
       "2      296802         3  I Don’t Wanna Live Forever (Fifty Shades Darke...   \n",
       "3      296803         4            Call On Me - Ryan Riback Extended Remix   \n",
       "4      296804         5                                   I Feel It Coming   \n",
       "\n",
       "         Artist  Streams                                                URL  \\\n",
       "0   Alan Walker   103886  https://open.spotify.com/track/0JiVRyTJcJnmlwC...   \n",
       "1  Clean Bandit    85990  https://open.spotify.com/track/5knuzwU65gJK7IF...   \n",
       "2          ZAYN    68706  https://open.spotify.com/track/3NdDpSvN911VPGi...   \n",
       "3       Starley    60334  https://open.spotify.com/track/78rIJddV4X0HkNA...   \n",
       "4    The Weeknd    56607  https://open.spotify.com/track/5GXAXm5YOmYT0kL...   \n",
       "\n",
       "         Date Region  \n",
       "0  2017-01-01     no  \n",
       "1  2017-01-01     no  \n",
       "2  2017-01-01     no  \n",
       "3  2017-01-01     no  \n",
       "4  2017-01-01     no  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norway.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Descriptions\n",
    "- `position` the rank of the song.<br>\n",
    "- `track_name` the of the song<br>\n",
    "- `artist` the artist<br>\n",
    "- `streams` the streams/day<br>\n",
    "- `url` the Spotify URL<br>\n",
    "- `date` the date the songs were streamed<br>\n",
    "- `region` country, by postal code<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filez = [norway, denmark, iceland, switzerland, finland, \n",
    "                       netherlands, canada, newzealand, sweden, australia, costarica, austria, \n",
    "                       unitedstates, ireland, germany, belgium, luxembourg, unitedkingdom, \n",
    "                       chile, brazil, czechrepublic, argentina, mexico, singapore, uruguay, guatemala,\n",
    "                       panama, france, taiwanprovinceofchina, spain, colombia, slovakia, malaysia, ecuador, elsalvador, poland, \n",
    "                       italy, japan,lithuania, latvia, bolivia, peru, estonia, turkey, paraguay, \n",
    "                       philippines, hungary, indonesia, dominicanrepublic, greece, portugal, honduras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy2017columns= [x.lower() for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\"..\",\".\") for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\".\",\"_\") for x in happy2017columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(countries_to_subset)):\n",
    "    countries_to_subset[i]=first_fifty(countries_to_subset[i])\n",
    "    countries_to_subset[i]=not18(countries_to_subset[i])\n",
    "    countries_to_subset[i]=cleancol(countries_to_subset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist</th>\n",
       "      <th>streams</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Otra Vez (feat. J Balvin)</td>\n",
       "      <td>Zion &amp; Lennox</td>\n",
       "      <td>6762</td>\n",
       "      <td>https://open.spotify.com/track/3QwBODjSEzelZyV...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Chantaje</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>6467</td>\n",
       "      <td>https://open.spotify.com/track/6mICuAdrwEjh6Y6...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Safari</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>5951</td>\n",
       "      <td>https://open.spotify.com/track/6rQSrBHf7HlZjtc...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>5760</td>\n",
       "      <td>https://open.spotify.com/track/3AEZUABDXNtecAO...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Shaky Shaky</td>\n",
       "      <td>Daddy Yankee</td>\n",
       "      <td>5054</td>\n",
       "      <td>https://open.spotify.com/track/58IL315gMSTD37D...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18084</th>\n",
       "      <td>46</td>\n",
       "      <td>Ahora Me Llama</td>\n",
       "      <td>Karol G</td>\n",
       "      <td>2031</td>\n",
       "      <td>https://open.spotify.com/track/11ZRYISCsAsLyyJ...</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18085</th>\n",
       "      <td>47</td>\n",
       "      <td>Too Good At Goodbyes</td>\n",
       "      <td>Sam Smith</td>\n",
       "      <td>1979</td>\n",
       "      <td>https://open.spotify.com/track/1mXVgsBdtIVeCLJ...</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18086</th>\n",
       "      <td>48</td>\n",
       "      <td>Krippy Kush</td>\n",
       "      <td>Farruko</td>\n",
       "      <td>1964</td>\n",
       "      <td>https://open.spotify.com/track/7FfpP3YZ6fOWMdx...</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18087</th>\n",
       "      <td>49</td>\n",
       "      <td>Me Emborrachare - Bachata Radio Edit</td>\n",
       "      <td>Grupo Extra</td>\n",
       "      <td>1959</td>\n",
       "      <td>https://open.spotify.com/track/3iGjLTDqokSuf6y...</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18088</th>\n",
       "      <td>50</td>\n",
       "      <td>Ahora Se</td>\n",
       "      <td>Lary Over</td>\n",
       "      <td>1956</td>\n",
       "      <td>https://open.spotify.com/track/3hfpB94cfVvdLAn...</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>hn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18089 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position                            track_name         artist  streams  \\\n",
       "0             1             Otra Vez (feat. J Balvin)  Zion & Lennox     6762   \n",
       "1             2                              Chantaje        Shakira     6467   \n",
       "2             3                                Safari       J Balvin     5951   \n",
       "3             4            Reggaetón Lento (Bailemos)           CNCO     5760   \n",
       "4             5                           Shaky Shaky   Daddy Yankee     5054   \n",
       "...         ...                                   ...            ...      ...   \n",
       "18084        46                        Ahora Me Llama        Karol G     2031   \n",
       "18085        47                  Too Good At Goodbyes      Sam Smith     1979   \n",
       "18086        48                           Krippy Kush        Farruko     1964   \n",
       "18087        49  Me Emborrachare - Bachata Radio Edit    Grupo Extra     1959   \n",
       "18088        50                              Ahora Se      Lary Over     1956   \n",
       "\n",
       "                                                     url       date region  \n",
       "0      https://open.spotify.com/track/3QwBODjSEzelZyV... 2017-01-01     hn  \n",
       "1      https://open.spotify.com/track/6mICuAdrwEjh6Y6... 2017-01-01     hn  \n",
       "2      https://open.spotify.com/track/6rQSrBHf7HlZjtc... 2017-01-01     hn  \n",
       "3      https://open.spotify.com/track/3AEZUABDXNtecAO... 2017-01-01     hn  \n",
       "4      https://open.spotify.com/track/58IL315gMSTD37D... 2017-01-01     hn  \n",
       "...                                                  ...        ...    ...  \n",
       "18084  https://open.spotify.com/track/11ZRYISCsAsLyyJ... 2017-12-31     hn  \n",
       "18085  https://open.spotify.com/track/1mXVgsBdtIVeCLJ... 2017-12-31     hn  \n",
       "18086  https://open.spotify.com/track/7FfpP3YZ6fOWMdx... 2017-12-31     hn  \n",
       "18087  https://open.spotify.com/track/3iGjLTDqokSuf6y... 2017-12-31     hn  \n",
       "18088  https://open.spotify.com/track/3hfpB94cfVvdLAn... 2017-12-31     hn  \n",
       "\n",
       "[18089 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_to_subset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "Here we are using the variable countries_to_subset to show whether our three previously defined functions are, in fact, working from the for-loop above the data frame above this text. The function **first_fifty** is subsetting the information of our selected countries by only taking the top fifty songs for each day. The function **not18** is subsetting the information of our selected countries by only taking observations from the year 2017 – excluding 2018. The function **cleancol** is changing column names to lowercase and replacing spaces with underscores; this will facilitate our use of certain Pandas function later in our analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if our change() method successfully changed the column names and dropped rows that contained NAN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74200, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296800</td>\n",
       "      <td>1</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>103886</td>\n",
       "      <td>https://open.spotify.com/track/0JiVRyTJcJnmlwC...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>296801</td>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>85990</td>\n",
       "      <td>https://open.spotify.com/track/5knuzwU65gJK7IF...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296802</td>\n",
       "      <td>3</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>68706</td>\n",
       "      <td>https://open.spotify.com/track/3NdDpSvN911VPGi...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296803</td>\n",
       "      <td>4</td>\n",
       "      <td>Call On Me - Ryan Riback Extended Remix</td>\n",
       "      <td>Starley</td>\n",
       "      <td>60334</td>\n",
       "      <td>https://open.spotify.com/track/78rIJddV4X0HkNA...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>296804</td>\n",
       "      <td>5</td>\n",
       "      <td>I Feel It Coming</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>56607</td>\n",
       "      <td>https://open.spotify.com/track/5GXAXm5YOmYT0kL...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Position                                         Track Name  \\\n",
       "0      296800         1                                              Alone   \n",
       "1      296801         2            Rockabye (feat. Sean Paul & Anne-Marie)   \n",
       "2      296802         3  I Don’t Wanna Live Forever (Fifty Shades Darke...   \n",
       "3      296803         4            Call On Me - Ryan Riback Extended Remix   \n",
       "4      296804         5                                   I Feel It Coming   \n",
       "\n",
       "         Artist  Streams                                                URL  \\\n",
       "0   Alan Walker   103886  https://open.spotify.com/track/0JiVRyTJcJnmlwC...   \n",
       "1  Clean Bandit    85990  https://open.spotify.com/track/5knuzwU65gJK7IF...   \n",
       "2          ZAYN    68706  https://open.spotify.com/track/3NdDpSvN911VPGi...   \n",
       "3       Starley    60334  https://open.spotify.com/track/78rIJddV4X0HkNA...   \n",
       "4    The Weeknd    56607  https://open.spotify.com/track/5GXAXm5YOmYT0kL...   \n",
       "\n",
       "         Date Region  \n",
       "0  2017-01-01     no  \n",
       "1  2017-01-01     no  \n",
       "2  2017-01-01     no  \n",
       "3  2017-01-01     no  \n",
       "4  2017-01-01     no  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(norway.shape)\n",
    "norway.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-79365a8b70b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcountries_to_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   5227\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5229\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5231\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5198\u001b[0m         \"\"\"\n\u001b[1;32m   5199\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5200\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5208\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5209\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1898\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1900\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m         )\n\u001b[1;32m   1902\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   1924\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main=pd.DataFrame()\n",
    "for country in countries_to_subset:\n",
    "    main=pd.concat([main,country])\n",
    "    main=main.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "Here, we are merging separate country dataframes for ease of later analysis using the function concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main=main.drop(columns=['unnamed:_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupcount=main.groupby('region').count()\n",
    "groupcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are grouping our data frame by regions. We are checking if the countries were concatenated properly. It should be noted that some countries, such as Luxembourg ('lu'), are missing a lot of data, which may impact our analyses using this data frame. As will be demonstrated later, Luxembourg will not be used for the final analyses given that it is neither one of the top or bottom 10 countries ([top and bottom ten countries](#topbot)). Looking at the other relevenat columns, however, can see that each country has rougly the same amount of songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if the spotipy sp.audiofeatures() method works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.audio_features('5aAx2yezTd8zXrkmtKl66Z')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now creating a method that will enable us to call the sp.audiofeatures() method. After some trial and error, in which we attempted to call use the Spotify API method on every URL on every song, in every row of the dataframe of each country, we realized that our method could never finish executing because the dataframes were so large and also that the Spotify API does have a limit on how many times you call their methods (we weren't allowed to use the Spotify API methods for a certain amount of time).\n",
    "\n",
    "We developed getunique() to first find all of the unique songs in each dataframe. We realized that for the top daily songs for a year, there is obviously a lot of repititon of songs. So using pd.unique() enabled us to find a way to reduce the amount of times we needed to call sp.audio_features(). This method returns a dataframe that is a collection of the audio features information for every unique song in each country's dataframe. \n",
    "\n",
    "After going to office hours and developing the following function, getunique(), we decided that using all 50+ countries was unnecessary for our analysis, and that the Spotify API could not handle so many calls. We decided to compare only the top 10 and bot 10 countries in happiness scores as is demonstrated below this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top and bottom 10 happiest countries <a name=\"topbot\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topbot=pd.DataFrame()\n",
    "topbot=pd.concat([topbot, happy.head(10)])\n",
    "topbot=pd.concat([topbot, happy.tail(10)])\n",
    "topbot=topbot.reset_index(drop=True)\n",
    "topbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main.groupby('region').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=main.region.unique()\n",
    "new=names[:10]\n",
    "new2=(names[len(names)-10:len(names)])\n",
    "finalnames=np.append(new,new2).tolist()\n",
    "\n",
    "finalsongs2017=main.loc[main['region'].isin(finalnames)]\n",
    "finalsongs2017=finalsongs2017.reset_index(drop=True)\n",
    "finalsongs2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are getting only the unique regions from our \"main\" data frame, which currently contains the song observations. We are collecting the list of unique region names, which we are then using to check if these region abbreviations are contained in our desired list of regions (finalnames). This step is necessary to subset the songs' information from only the top and bottom happiest countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2017['country']=finalsongs2017['region']\n",
    "finalsongs2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "Rather than the name of all of our top and bottom 10 countries being completely spelled out, we are using the region abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict for mapping\n",
    "region_dict={'no':\"Norway\", 'dk':\"Denmark\", 'ch':\"Switzerland\", 'fi':\"Finland\", 'nl':\"Netherlands\", 'se':\"Sweden\", 'ee':\"Estonia\", 'hu':\"Hungary\", 'id':\"Indonesia\", 'is':\"Iceland\", 'ca':\"Canada\",\n",
    "       'nz':\"New Zealand\", 'au':\"Australia\", 'tr':\"Turkey\", 'py':\"Paraguay\", 'ph':\"Philippines\", 'do':\"Dominican Republic\", 'gy':\"Greece\", 'pt':\"Portugal\", 'hn':\"Honduras\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2017.country=finalsongs2017['country'].map(region_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "The code above is creating a dictionary of the regions and their respective – top 10 and bottom 10 – countries. We are mapping this to the country column of our finalsongs2017 data dframe. As opposed to having just the abbreviations for the country, we want to have one column with just the region abbreviation and another with the full name of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify2017=topbot.merge(finalsongs2017, on=\"country\")\n",
    "spotify2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are merging our top and bottom 10 countries on the **country** column of our main data frame containing all songs (finalsongs2017). This new data frame will now be under the variable name spotify2017.\n",
    "\n",
    "*Note: this **spotify2017** data frame contains our bottom 10 and top 10 happiest countries with their respective song observations. There are 50 observations for each day over the 2017 year for all 20 countries.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getunique(dataframe):\n",
    "    '''\n",
    "    creates a dataframe containing the audio features of songs, all scores, which are floats. These audio features are provided by the Spotify API.\n",
    "    parameters: a dataframe with a 'url' column, which contains valid links to a song on Spotify.\n",
    "    returns: a dataframe with the audio features as columns, the observations being\n",
    "    ONLY for the unique songs in the provided dataframe. Because the dataframe contains several repeated songs,\n",
    "    we will avoid calling the API on duplicated songs because we will get a \"max entries error\"\n",
    "    '''\n",
    "    audiofeatures=pd.DataFrame({'danceability':[], 'energy':[],'key':[], 'loudness':[], 'mode':[], 'speechiness':[],'acousticness':[], 'instrumentalness':[],'liveness':[], 'valence':[], 'tempo':[], 'type':[],'id':[], 'uri':[],'track_href':[], 'analysis_url':[], 'duration_ms':[], 'time_signature':[]})\n",
    "    dataframe.drop_duplicates(subset =\"track_name\",\n",
    "                     keep = 'first', inplace = True,) #drop duplicate songs\n",
    "    dataframe=dataframe.reset_index(drop=True)\n",
    "    for row in range(len(dataframe)):\n",
    "            url=dataframe.loc[row,'url']\n",
    "            sub=url.rindex('/')\n",
    "            idurl=url[sub+1:]\n",
    "            newrow=sp.audio_features(idurl)[0]\n",
    "            audiofeatures=audiofeatures.append(newrow,ignore_index=True)\n",
    "    audiofeatures['track_name']=dataframe['track_name']\n",
    "    audiofeatures['happiness_score']=dataframe['happiness_score']\n",
    "    audiofeatures['happiness_rank']=dataframe['happiness_rank']\n",
    "    audiofeatures['country']=dataframe['country']\n",
    "    return audiofeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify=getunique(spotify2017.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are constructing a new function (**getunique**) to create a new data frame that contains all relevant audio features, which we plan on using for our analyses. As noted in the function description, we are only using the unique functions of our data frame, given that many songs may repeat on various days throughout the year. As such, we do not want to have duplicates of the same song as these may jeopardize our analyses findings by placing more weight on certain observations. These features will be saved in the variable **uniquespotify**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unecessary columns\n",
    "uniquespotify=uniquespotify.drop(['analysis_url','time_signature','track_href','uri','type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder columns\n",
    "uniquespotify=uniquespotify[['country', 'happiness_score', 'happiness_rank','track_name', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "       'id', 'duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are changing the order to our data frame columns to create a logical order of the songs, the happiness information, and the track information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2017=finalsongs2017.merge(uniquespotify[['danceability','track_name' ,'happiness_score', 'happiness_rank', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'id', 'duration_ms']], how=\"left\",on='track_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2017=finalsongs2017[['country','happiness_score', 'happiness_rank','date','position', 'track_name', 'artist', 'streams', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness','instrumentalness', 'liveness', 'valence', 'tempo', 'id','duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are merging our **finalsongs2017** data frame, which contains the relevant songs for the bottom 10 and top 10 happiest countries, with the features we just extracted from all songs. This will allow us to subset only the songs from these countries, and their respective days. Our updated **finalsongs2017** will now contain these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2019=pd.read_csv(\"spotify2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nans for later\n",
    "finalsongs2019.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Finland', 'Denmark', 'Norway', 'Iceland', 'Netherlands',\n",
       "       'Switzerland', 'Sweden', 'New Zealand', 'Canada', 'Austria',\n",
       "       'Portugal', 'Philippines', 'Hong Kong', 'Dominican Republic',\n",
       "       'Turkey', 'Malaysia', 'Greece', 'Indonesia', 'Vietnam',\n",
       "       'South Africa'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(finalsongs2019['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>Unnamed: 0  position  streams        da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Unnamed: 0  position  streams        da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Greece</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Unnamed: 0  position  streams        da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Unnamed: 0  position  streams        da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Unnamed: 0  position  streams        da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Unnamed: 0  position  streams        d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               country                                                  1\n",
       "0              Austria          Unnamed: 0  position  streams        d...\n",
       "1               Canada          Unnamed: 0  position  streams        d...\n",
       "2              Denmark         Unnamed: 0  position  streams        da...\n",
       "3   Dominican Republic          Unnamed: 0  position  streams        d...\n",
       "4              Finland         Unnamed: 0  position  streams        da...\n",
       "5               Greece          Unnamed: 0  position  streams        d...\n",
       "6            Hong Kong          Unnamed: 0  position  streams        d...\n",
       "7              Iceland         Unnamed: 0  position  streams        da...\n",
       "8            Indonesia          Unnamed: 0  position  streams        d...\n",
       "9             Malaysia          Unnamed: 0  position  streams        d...\n",
       "10         Netherlands         Unnamed: 0  position  streams        da...\n",
       "11         New Zealand          Unnamed: 0  position  streams        d...\n",
       "12              Norway         Unnamed: 0  position  streams        da...\n",
       "13         Philippines          Unnamed: 0  position  streams        d...\n",
       "14            Portugal          Unnamed: 0  position  streams        d...\n",
       "15        South Africa          Unnamed: 0  position  streams        d...\n",
       "16              Sweden          Unnamed: 0  position  streams        d...\n",
       "17         Switzerland          Unnamed: 0  position  streams        d...\n",
       "18              Turkey          Unnamed: 0  position  streams        d...\n",
       "19             Vietnam          Unnamed: 0  position  streams        d..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topbot=pd.DataFrame()\n",
    "test=pd.DataFrame(finalsongs2019.groupby(\"country\"))\n",
    "topbot=pd.concat([topbot, test])\n",
    "# topbot=pd.concat([topbot, happy.tail(10)])\n",
    "topbot=topbot.reset_index(drop=True)\n",
    "topbot\n",
    "topbot=topbot.rename(columns={0:\"country\"})\n",
    "topbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topbot=topbot.merge(finalsongs2019[['happiness_score', 'happiness_rank','country']], how=\"left\",on='country')\n",
    "topbot.drop([1],axis=1,inplace=True)\n",
    "topbot.dropduplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII Exploratory Data Analysis <a name=\"exploratory\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topfinal=pd.DataFrame()\n",
    "split=finalsongs2019[finalsongs2019['country']=='Portugal'].index[0]\n",
    "topfinal=finalsongs2019[:split]\n",
    "botfinal=finalsongs2019[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling=pd.DataFrame()\n",
    "topfinalrolling = topfinal.groupby('country', sort=False)[['danceability']].rolling(window=14).mean().reset_index()\n",
    "# rolling['avg_danceability']=finalsongs['danceability'].rolling(14).mean()\n",
    "topfinalrolling=topfinalrolling.merge(topbot[['country','happiness_score']], how='left',on='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "This is the part where we conduct our rolling average for our updated data frame **finalsongs2017**. In particular, we are using Estonia as an index position – given that this is the first of the bottom 10 countries – in order to consider our bottom and top final observations – or split, for the purpose of then grouping by country. We are conducting the rolling average of two weeks for danceability, which we are then merging on the country column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling2=topfinal.groupby('country', sort=False)[['valence']].rolling(window=14).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are performing the rolling average for two weeks for valence. \n",
    "\n",
    "In sum, we will have conducted the rolling average of two weeks for both danceability and valence, which we believe may best represent the overall \"tune\" – or how happy a song is – for a given country. \n",
    "\n",
    "*Note: We believe a rolling average is necessary here as it will allow us to understand potential trends with respect to the songs individuals in these countries listen to over the course of 14 days. With this in mind, we are making the assumption that **14 days** will be sufficient to generalize the musical trends over the course of one year – and therein, how happy the music the people of a country listen to. Furthermore, we are assuming the results of this window will allow us to gain more insight into the overall kind of music countries listen to, and how this relates to their happiness ranking as a country.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topfinalrolling['valence']=rolling2['valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling2['danceability']=topfinalrolling['danceability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topfinalrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "This new **topfinalrolling** will contain the rolling average of two weeks for **danceability** and **valence** for only the **top 10 countries.**\n",
    "\n",
    "*Note: The first few observations for our rolling average will be NaN values given that these are the observations being taken to compute the very first rolling average*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botfinalrolling=botfinal.groupby('country', sort=False)[['danceability']].rolling(window=14).mean().reset_index()\n",
    "# rolling['avg_danceability']=finalsongs['danceability'].rolling(14).mean()\n",
    "botfinalrolling=botfinalrolling.merge(topbot[['country','happiness_score']], how='left',on='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling3=botfinal.groupby('country', sort=False)[['valence']].rolling(window=14).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botfinalrolling['valence']=rolling3['valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling3['danceability']=botfinalrolling['danceability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botfinalrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description for Work Above\n",
    "We are now performing the rolling average for the bottom ten countries. We are still only focusing on danceability and valence. This information will now be saved in the variable name **botfinalrolling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter=sns.relplot(x='danceability', y='valence', hue='country', data=topfinalrolling, alpha=0.3)\n",
    "plt.title(\"Rolling Average (2 weeks) Happiness and Valence \\n Top 10 Happy Countries\")\n",
    "plt.tight_layout\n",
    "plt.figure(figsize = (100, 45))\n",
    "scatter.set(xlim=(.01,1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above Graph Description\n",
    "This graph is illustrating our rolling average of two weeks for danceability and valence for the top 10 happiest countries, which can be seen on the right.\n",
    "\n",
    "*Interpretation*<p>As can be seen, the majority of songs for the top 10 countries tend to center around a 0.6 danceability score. This could mean the top 10 happiest countries do not show much disparity in the songs their people listen to. That is, their songs tend to be equally as \"danceable.\" For the purpose of our analysis, this means that our top 10 happiest countries listen to midly danceable music. With respect to valence, however, there appears to be a wider range of valence scores that are covered by the songs these top countries listen to. As such, this may be a fruitless metric in attempting to understand the music these top countries listen to. This would imply valence scores largely vary by song, contrary to danceability, which is more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter=sns.relplot(x='danceability', y='valence', hue='country', data=botfinalrolling, alpha=0.3)\n",
    "plt.title(\"Rolling Average (2 weeks) Happiness and Valence \\n Bottom 10 Countries\")\n",
    "plt.tight_layout\n",
    "plt.figure(figsize = (100, 45))\n",
    "scatter.set(xlim=(.01,1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topavg=topfinal.groupby('country').mean()\n",
    "scatter=sns.relplot(x='danceability', y='valence', hue=topavg.index, data=topavg)\n",
    "plt.title(\"Rolling Average Mean (2 weeks) Happiness and Valence \\n Top 10 Happy Countries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above Graph Description\n",
    "Having the rolling average of all observations was relatively difficult to discern any trends, given the overlap of countries. As such, this graph represents the mean of the rolling average of two weeks by country. \n",
    "\n",
    "*Interpretation*\n",
    "<p> Contrary to our expectations, the danceability and valence scores and valence do not show a linear relationship with respect to the top countries in ascending order (i.e., the happiest countries do not coincidentally have the highest danceability or valence scores). There appears to be a wide range of danceability and valence scores for all top countries, which may signify no notable relationship between the danceability and valence scores and the happiness scores of the top countries. Again, we are under the assumption this two-week period is sufficient to draw relevant conclusions (roughly 700 song observations per country).<p>\n",
    "\n",
    "*Note:The key does not show the countries in their respective order from highest to least happiest country*\n",
    "\n",
    "**This is the correct order for the top 10 countries from happiest to least happy**\n",
    "1. Finland\n",
    "2. Denmark\n",
    "3. Norway\n",
    "4. Iceland\n",
    "5. Netherlands\n",
    "6. Switzerland\n",
    "7. Sweden \n",
    "8. New Zealand\n",
    "9. Canada\n",
    "10. Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botavg=botfinal.groupby('country').mean()\n",
    "scatter=sns.relplot(x='danceability', y='valence', hue=botavg.index, data=botavg)\n",
    "plt.title(\"Rolling Average (2 weeks) Happiness and Valence \\n Bottom 10 Happy Countries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above Graph Description\n",
    "This graph represents the **mean** rolling average of the bottom 10 countries.\n",
    "\n",
    "*Interpretation*\n",
    "<p> There appears to be a more linear relationship between the danceability and valence scores and the 10 bottom happiest countries. By the same token, some of the lowest ranking countries – in terms of happiness– have relatively high danceability scores, as in the case of Honduras and the Dominican Republic. These, however, can be interpreted as potential anomalies given the other countries do not show a similar pattern. Given that there is a wide range of danceability and valence scores for these countries, we can fairly conclude there is no notable relationship between the danceability and valence and the happiness score of a country. We are still under the assumption a two-week period is sufficient to draw larger conclusions for the overall musical trend of the country. All in all, the rolling average of the top and bottom 10 countries highlights there is no meaningful relationship between how happy a country is and how danceable the music the people of these countries listen to. As it pertains to our research question, this supports the argument that there is no particular relationship between how happy a country is and the danceability and valence of its music. Other features, however, may be at play – independent of danceability and valence – to still assess the signficiance of music on a country's overall happiness rank.<p>\n",
    "    \n",
    "*Note: The key does not show the countries in their respective order from highest to least happiest countries*\n",
    "    \n",
    "**This is the correct order for the bottom 10 happiest countries from happiest to least happy**\n",
    "1. Malaysia\n",
    "2. Greece\n",
    "3. Morocco\n",
    "4. Indonesia\n",
    "5. Vietnam\n",
    "6. Bulgaria\n",
    "7. South Africa\n",
    "8. Ukraine \n",
    "9. Egypt\n",
    "10. India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are printing the original happy dataframe but with the average spotify audio feature score for their top daily songs in its own column. This will be useful for any computations or graphic we try and make later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "botavg.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topavg.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Description for Work Above\n",
    "We understand there were not many significant trends using rolling average for danceability and valence as it pertains to the top and bottom ten countries. Therefore, we will see which features show the highest correlation with happiness score and happiness rank and attempt to draw our following analysis from these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(topavg.corr(), center=0, cmap='coolwarm')\n",
    "plt.title(\"Correlation of Rolling Average (Two Weeks) \\n Top 10 Happy Countries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(botavg.corr(), center=0, cmap='coolwarm')\n",
    "plt.title(\"Correlation of Rolling Average (Two Weeks) \\n Bottom 10 Happy Countries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(botavg['happiness_score'],botavg['valence'])\n",
    "plt.title(\"Rolling Average Valence \\n Bottom 10 Happy Countries\")\n",
    "plt.xlabel(\"Happiness Score\")\n",
    "plt.ylabel(\"Valence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(topavg['happiness_score'],topavg['valence'])\n",
    "plt.title(\"Rolling Average Valence \\n Top 10 Happy Countries\")\n",
    "plt.xlabel(\"Happiness Score\")\n",
    "plt.ylabel(\"Valence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "#plt.title(\"Happiness Score and Danceability \\n Top 10 and Bottom 10 Happy Countries\")\n",
    "sns.lineplot(x='happiness_score', y='danceability', data=topfinal, ax=axs[0])\n",
    "sns.lineplot(x='happiness_score', y='danceability', data=botfinal, ax=axs[1])\n",
    "axs[0].title.set_text('Top 10 Happy Countries')\n",
    "axs[1].title.set_text('Bottom 10 Happy Countries')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "sns.lineplot(x=\"happiness_score\",y=\"danceability\",\n",
    "             label=\"Top 10 Countries\", data=topfinal,\n",
    "             ci=None)\n",
    "\n",
    "sns.lineplot(x=\"happiness_score\",y=\"danceability\",\n",
    "             label=\"Bottom 10 Countries\",\n",
    "             data=botfinal,\n",
    "             ci=None)\n",
    "\n",
    "plt.xlabel(\"Happiness Score\", size=14)\n",
    "plt.ylabel(\"Danceability\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "sns.lineplot(x=\"happiness_score\",y=\"valence\",\n",
    "             label=\"Top 10 Countries\", data=topfinal,\n",
    "             ci=None)\n",
    "\n",
    "sns.lineplot(x=\"happiness_score\",y=\"valence\",\n",
    "             label=\"Bottom 10 Countries\",\n",
    "             data=botfinal,\n",
    "             ci=None)\n",
    "\n",
    "plt.xlabel(\"Happiness Score\", size=14)\n",
    "plt.ylabel(\"valence\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happiness Score and Danceability\n",
    "At a first glance, we observe no meaningful relationship between the top 10 countries and the bottom 10 countries as it relates to how danceable the songs these countries listen to are. However, there appears to be a notable trend for the top 10 countries in that the danceability score remains above 0.60 despite there being a decrease at ~7.3 and ~7.5. In comparing both top and bottom 10 countries, however, we cannot draw a truly significant relationship given that many countries from the bottom 10 have relatively high danceability scores – in particular at ~5.3 and ~5.5 – as those danceability scores from the top 10 countries. Additionally, it is worth noting that many of the bottom 10 countries with happiness scores in the range ~5.6-7.3 have constant danceability scores ranging from ~0.63-0.67. We assume this can be due to a commitment to specific, potentially esoteric genres in some countries. In other words, some of these \"mildly\" happy countries enjoy specific genres, and thus their people listen primarily only to these countries, which coincidentally happen to have \"mildly\" danceable scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy2020=pd.read_csv('happiness2020.csv')\n",
    "happy2020=happy2020[['Country name','Ladder score']]\n",
    "happy2020['happiness_rank']=happy2020.index.copy()\n",
    "happy2=happy2020.happiness_rank.to_numpy()\n",
    "happy2=happy2+1\n",
    "happy2020['happiness_rank']=happy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2020= [x.lower() for x in happy2020.columns] \n",
    "cols2020= [x.replace(\" \",\"_\") for x in cols2020] \n",
    "happy2020.columns=cols2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII Training a model to predict happiness scores in 2020 <a name=\"training\"></a>\n",
    "In a separate notebook, we scraped the top songs from 2020 per day per country from the Spotify charts website (this will be explained later). In short, we have a .csv file that is like the `finalsongs2017` dataframe but for 2020 song data. It contains the top 50 songs per day for the whole year for the top 10 and bot 10 happiest countries in 2020. These countries were identified using the Kaggle dataset for the 2020 happiness ranking.\n",
    "We want to create a model from the 2017 data to predict the happiness scores of the songs in 2020, and we will compare the accuracy of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify2020=pd.read_csv('spotify2020.csv')\n",
    "names2020=spotify2020.region.unique()\n",
    "\n",
    "\n",
    "finalsongs2020=spotify2020.loc[spotify2020['region'].isin(names2020)]\n",
    "finalsongs2020=finalsongs2020.reset_index(drop=True)\n",
    "finalsongs2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simliar protocols from the 2017 dataset were done to merge the 2020 dfs\n",
    "#making a country col to merge for later\n",
    "finalsongs2020['country']=finalsongs2020['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict2020={'fi':\"Finland\", 'dk':\"Denmark\", 'ch':\"Switzerland\", 'is':\"Iceland\", 'no':\"Norway\", 'nl':\"Netherlands\", 'se':\"Sweden\", 'nz':\"New Zealand\", 'at':\"Austria\", 'ca':\"Canada\", 'do':\"Dominican Republic\",\n",
    "       'gr':\"Greece\", 'my':\"Malaysia\", 'vn':\"Vietnam\", 'id':\"Indonesia\", 'tr':\"Turkey\", 'ma':\"Morocco\", 'za':\"South Africa\", 'eg':\"Egypt\", 'in':\"India\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2020.country=finalsongs2020['country'].map(region_dict2020)\n",
    "finalsongs2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries2020=happy2020.country_name\n",
    "spotifycountries2020=pd.unique(finalsongs2020['country'])\n",
    "happy2020=happy2020.loc[happy2020['country_name'].isin(spotifycountries2020)]\n",
    "happy2020=happy2020.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df with happiness ranking for 2020\n",
    "happy2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more cleaning and merging\n",
    "happy2020.rename({\"country_name\":\"country\"}, axis=\"columns\", inplace=True)\n",
    "happy2020.rename({\"ladder_score\":\"happiness_score\"}, axis=\"columns\", inplace=True)\n",
    "spotify2020=happy2020.merge(finalsongs2020, on=\"country\")\n",
    "spotify2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is basically the same routine as the 2017 to get a dataframe with the audiofeatures of the Spotify 2020 songs. We have commented out the code because we saved the .csv file in our first time running it and now pd.read_csv rather than running the cells again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling getunique again\n",
    "# uniquespotify2020=getunique(spotify2020.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniquespotify2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save2020=uniquespotify2020.copy() #saving df in case of mistakes\n",
    "# uniquespotify2020=uniquespotify2020.drop(['analysis_url','time_signature','track_href','uri','type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniquespotify2020=uniquespotify2020[['country', 'happiness_score', 'happiness_rank','track_name', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "#        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "#        'id', 'duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020=finalsongs2020.merge(uniquespotify2020[['danceability','track_name' ,'happiness_score', 'happiness_rank', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "# 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'id', 'duration_ms']], how=\"left\",on='track_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "# finalsongs2020.drop('a',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020=finalsongs2020[['country','happiness_score', 'happiness_rank','date','position', 'track_name', 'artist', 'streams', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness','instrumentalness', 'liveness', 'valence', 'tempo', 'id','duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020.to_csv('finalsongs2020')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalsongs2020=finalsongs2020.copy()\n",
    "# finalsongs2020.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2020=pd.read_csv(\"finalsongs2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcol=['pred_danceability', 'pred_energy', 'pred_key',\n",
    "       'pred_loudness', 'pred_mode', 'pred_speechiness', 'pred_acousticness', 'pred_instrumentalness',\n",
    "       'pred_liveness', 'pred_valence', 'pred_tempo', 'pred_duration_ms']\n",
    "index=0\n",
    "coeff_determination=[]\n",
    "predictions=pd.DataFrame()\n",
    "for feature in ['danceability', 'energy', 'key',\n",
    "       'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "       'liveness', 'valence', 'tempo', 'duration_ms']:\n",
    "    linear_model=lr()\n",
    "    linear_model.fit(finalsongs2020[[feature]],finalsongs2020['happiness_score'])\n",
    "    preds=linear_model.predict(finalsongs2020[['happiness_score']])\n",
    "    \n",
    "    predictions[newcol[index]]=preds\n",
    "    index=index+1\n",
    "    coeff_determination.append(round(linear_model.score(finalsongs2020[[feature]],finalsongs2020['happiness_score']),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['country']=finalsongs2020['country']\n",
    "predictions=predictions.merge(happy2020,on='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.drop_duplicates(subset =\"country\",\n",
    "                     keep = 'first', inplace = True,)\n",
    "predictions=predictions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.rename({\"happiness_score\":\"obs_happiness_score\"}, axis=\"columns\", inplace=True)\n",
    "\n",
    "\n",
    "first_column = predictions.pop('obs_happiness_score')\n",
    "predictions.insert(0, 'obs_happiness_score', first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = predictions.pop('country')\n",
    "predictions.insert(0, 'country', first_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataframe `predictions` are the predicted happiness scores of 2020 based on the top songs of their respected countries. `obs_happiness_score` is the real happiness score, and the other columns are the predicted happiness score based on the audiofeature. For example, `pred_danceability` is the predicted happiness scores for each country based on a country's danceability scores for their top songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff=pd.DataFrame({'danceability':[0.01], 'energy':[0.0], 'key':[0.0], 'loudness':[0.0], 'mode':[0.0], 'speechiness':[0.0], 'acousticness':[0.03], 'instrumentalness':[0.0],\n",
    "                    'liveness':[0.01], 'valence':[0.01], 'tempo':[0.0], 'duration_ms':[0.04]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oddly`duration_ms` (the length of a song) has the highest coefficient of determination. According to the coefficient of determination, around 4% of the variation in the happiness scores of different countries can be explained by the length of a song. However, this is extremely small. As we can see with these determination scores and the following graphs, the model we created for a single variable is not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalsongs[[feature]],finalsongs['happiness_score']\n",
    "\n",
    "plt.scatter(predictions['country'],predictions['pred_danceability'],c='blue', label='Predictions')\n",
    "plt.scatter(predictions['country'],predictions['obs_happiness_score'],c='red', label='Observations')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Predicted and Observed Happiness Scores for 2020')\n",
    "plt.xlabel('country')\n",
    "plt.ylabel('happiness score based on danceability')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions['country'],predictions['pred_duration_ms'],c='blue', label='Predictions')\n",
    "plt.scatter(predictions['country'],predictions['obs_happiness_score'],c='red', label='Observations')\n",
    "plt.legend()\n",
    "plt.xlabel('country')\n",
    "plt.ylabel('happiness score based on duration_ms')\n",
    "plt.tight_layout()\n",
    "plt.title('Comparison of Predicted and Observed Happiness Scores for 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions['country'],predictions['pred_valence'],c='blue', label='Predictions')\n",
    "plt.scatter(predictions['country'],predictions['obs_happiness_score'],c='red', label='Observations')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Predicted and Observed Happiness Scores for 2020')\n",
    "plt.xlabel('country')\n",
    "plt.ylabel('happiness score based on valence')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_danceability==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for danceability: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_energy==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for energy: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_key==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for key: \"+(str(accuracy)+'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_loudness==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for loudness: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_mode==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for mode: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_speechiness==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for speechiness: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_acousticness==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for acousticness: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_instrumentalness==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for instrumentalness: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_liveness==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for liveness: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_valence==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for valence: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_tempo==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"accuracy for tempo: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=round((len(predictions.query('pred_duration_ms==obs_happiness_score'))/len(predictions))*100, 1)\n",
    "print(\"duration_ms for energy: \"+str(accuracy)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lin_model=lr()\n",
    "multi_lin_model.fit(finalsongs2020[['energy', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']],finalsongs2020['happiness_score'])\n",
    "\n",
    "for x, feature in enumerate(['energy', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']):\n",
    "    print('{} coefficient: {:.2f}'.format(feature, multi_lin_model.coef_[x]))\n",
    "\n",
    "print('r^2 of the model: '+str(round((multi_lin_model.score(finalsongs2020[['energy', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']],finalsongs2020['happiness_score'])),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clist=pd.unique(finalsongs2020.country)\n",
    "clist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multhap=pd.DataFrame()\n",
    "mult_pred_happy=[]\n",
    "for country in clist:\n",
    "    country_pred=finalsongs2020[finalsongs2020['country']==country]\n",
    "    mean_scores=pd.DataFrame(country_pred.mean()).T\n",
    "    mult_pred=multi_lin_model.predict(mean_scores[['energy', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms']])\n",
    "    mult_pred_happy.append(mult_pred[0])\n",
    "# multhap['country']=clist\n",
    "# multhap['pred_happiness_score']=mult_pred_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multhap=pd.DataFrame()\n",
    "multhap['country']=clist\n",
    "multhap['pred_happiness_score']=mult_pred_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multhap['obs_happiness_score']=predictions['obs_happiness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multhap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(multhap['country'],multhap['pred_happiness_score'],c='blue', label='Predictions')\n",
    "plt.scatter(multhap['country'],multhap['obs_happiness_score'],c='red', label='Observations')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Predicted and Observed Happiness Scores for 2020')\n",
    "plt.xlabel('country')\n",
    "plt.ylabel('happiness score based on energy, acousticness, liveness, valence, tempo, duration_ms')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at these graphs, there does not seem to be any clear relationship between the song features and the happiness score of a country. Though multiple regression seems to be a better predictor than single linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX Sources <a name=\"sources\"></a>\n",
    "**Source Code** <br>\n",
    "GitHub  Repository: https://github.com/Albina-C/INFO-2950-Project \n",
    "\n",
    "**Appendix** <br>\n",
    "Web-Scraping Notebook: https://drive.google.com/file/d/1uBqzwdF6lvnUKfJwuds8RsQsXBPl1LFz/view?usp=sharing \n",
    "\n",
    "**Acknowledgements** <br>\n",
    "Spotipy: https://spotipy.readthedocs.io/en/2.18.0/ \n",
    "<br>\n",
    "Cloudscraper: https://pypi.org/project/cloudscraper/ \n",
    "<br>\n",
    "Web-Scraping Code Inspiration: https://gist.github.com/hktosun/d4f98488cb8f005214acd12296506f48 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X Questions for Reviewers <a name=\"questions\"></a>\n",
    "1. What parts of our notebook need more explanation (code, graphs, cleaning, etc)?\n",
    "2. After seeing that we have concluded that there is very little relation between a country's happiness and their most popular songs on Spotify, is there something you are curious to know (for us to explore)? What would you like to see more of and what would you like to see less of?\n",
    "3. We plan on not using the Spotify Kaggle data sets for the final project, so some of the data description questions will not be applicable. How should we go about answering these questions when we construct our dataset?\n",
    "4. Is training only on 2020 (given this is what we have) sufficient to refute our claim that these features are not predictive of happiness score?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
