{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Research Question\n",
    "#### Is there a relationship in how happy a country is and the music the people of this country listen to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Attempting to understand what makes a country \"happy\" is often attributable to freedom, social support, life expectancy, health, among others. However, the music the people of said country listen to is often not one of these factors given its weak relationship and versatility across the world. Although our research question does not attempt to attribute the type of music a country listens to to its happiness index score (it would be difficult to establish given reverse causality), we are more interested in observing what trends are apparent in some of the happiest and less happiest countries with respect to the music they listen to. \n",
    "\n",
    "#### Some additional questions that we are seeking to answer...\n",
    "- What songs are there (if any) that are popular in all countries regardless of happiness rank?\n",
    "- Are there genres/styles of music that are consistent with \"happy\" countries? With \"sad\" countries? For example, can we expect happy countries to listen to more pop music than sad ones?\n",
    "- What are the characteristics of songs that are popular in happy countries? Fast or slow tempo? Live vocals or autotune?\n",
    "- Can we find more instances of happy songs in sad countries than sad songs in happy countries?\"\n",
    "\n",
    "#### The datasets we will be using:\n",
    "\n",
    "All the datasets were found on Kaggle.\n",
    "<br>\n",
    "1. [\"World Happiness Report 2017\"](https://www.kaggle.com/unsdsn/world-happiness) by Sustainable Development Solutions Network\n",
    "<br>\n",
    "The Sustainable Development Solutions Network ranks 155 countries by their happiness levels, and calculates a \"happiness score\" for each country using six factors: economic production, social support, life expectancy, freedom, absence of corruption, and generosity. This dataset is for the year 2017.\n",
    "2. [\"Spotify's Worldwide Daily Song Ranking\"](https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking) by Kaggle user\n",
    "<br>\n",
    "For each country in 54 countries, this dataset provides the top 200 songs per day in the year of 2017 (January 1, 2017 to January 9, 2018). \n",
    "- *Note: the Kaggle description says 53 countries, but we found 54 countries. Perhaps the description was not updated when the dataset was.*\n",
    "3. [\"Spotify Web API\"](https://developer.spotify.com/documentation/web-api/reference/) \n",
    "<br>\n",
    "This dataset contains contains characteristics of songs on Spotify. Spotify calculates and gives scores for their songs. Some of these values include scores for danceability, beats per minute (bpm), and liveness (the likeliness that the song is a live recording)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spotipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-becfeadb0f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Spotify API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspotipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspotipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpotifyClientCredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spotipy'"
     ]
    }
   ],
   "source": [
    "#importing relevant packages\n",
    "import requests #package for http requests\n",
    "import bs4 # package for html parsing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#Spotify API\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import seaborn as sns\n",
    "\n",
    "#cid AND secret IS SPECIFIC TO EACH USER. SO EVA, ESTELLE, AND CESAR SHOULD HAVE DIFFERENT CID AND SECRET. \n",
    "# Do we have to each make our own cid and secret? or can we just use mine?\n",
    "#Eva's:\n",
    "# cid = '24ac9ca75f06477ca560d8c71807dd9e'\n",
    "# secret = '086fa8a3b056408e9bce55e3245c4af1'\n",
    "\n",
    "#Estelle's:\n",
    "cid='c10b42de14134edfb7e9cafa42fc48a2'\n",
    "secret='b41981d56a924e65a079138f9272e8de'\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager\n",
    "=\n",
    "client_credentials_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data cleaning for \"World Happiness Report\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"World Happiness Report 2017\"\n",
    "happy2017=pd.read_csv(\"2017.csv\")\n",
    "#cleaning up col names\n",
    "happy2017columns= happy2017.columns\n",
    "happy2017columns= [x.lower() for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\"..\",\".\") for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\".\",\"_\") for x in happy2017columns]\n",
    "happy2017.columns= happy2017columns\n",
    "happy2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all columns except for \"country\", \"happiness_rank\", and \"happiness_score\"\n",
    "happy2017=happy2017.iloc[:,:3]\n",
    "happy2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\"Spotify's Worldwide Song Ranking\"\n",
    "#This .csv file was so big that not only could we not push it to GitHub, but it was also difficult to load the file on Sheets.\n",
    "#Locally on her own computer, Eva split up data.csv into 53 individual .csv files by country so that we work with the data.\n",
    "\n",
    "#allspotifydata=pd.read_csv(\"data.csv\")\n",
    "#countries=pd.unique(allspotifydata['Region'])\n",
    "\n",
    "#allcountries=[]\n",
    "#for country in countries:\n",
    "    #allcountries.append(allspotifydata[allspotifydata['Region']==country])\n",
    "\n",
    "#count=1\n",
    "#for df in allcountries:\n",
    "    #name='country'+str(count)+'.csv'\n",
    "    #df.to_csv(r'C:\\Users\\Eva\\Downloads\\country'+str(count)+'.csv')\n",
    "    #count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the 54 country .csv split from data.csv\n",
    "argentina=pd.read_csv(\"argentina.csv\")\n",
    "australia=pd.read_csv(\"australia.csv\")\n",
    "austria=pd.read_csv(\"austria.csv\")\n",
    "belgium=pd.read_csv(\"belgium.csv\")\n",
    "bolivia=pd.read_csv(\"bolivia.csv\")\n",
    "brazil=pd.read_csv(\"brazil.csv\")\n",
    "canada=pd.read_csv(\"canada.csv\")\n",
    "chile=pd.read_csv(\"chile.csv\")\n",
    "colombia=pd.read_csv(\"colombia.csv\")\n",
    "costarica=pd.read_csv(\"costarica.csv\")\n",
    "czechrepublic=pd.read_csv(\"czechrepublic.csv\")\n",
    "denmark=pd.read_csv(\"denmark.csv\")\n",
    "dominicanrepublic=pd.read_csv(\"dominicanrepublic.csv\")\n",
    "ecuador=pd.read_csv(\"ecuador.csv\")\n",
    "elsalvador=pd.read_csv('elsalvador.csv')\n",
    "estonia=pd.read_csv('estonia.csv')\n",
    "finland=pd.read_csv(\"finland.csv\")\n",
    "france=pd.read_csv(\"france.csv\")\n",
    "germany=pd.read_csv(\"germany.csv\")\n",
    "Global=pd.read_csv('global.csv')\n",
    "greece=pd.read_csv('greece.csv')\n",
    "guatemala=pd.read_csv(\"guatemala.csv\")\n",
    "honduras=pd.read_csv(\"honduras.csv\")\n",
    "hongkong=pd.read_csv(\"hongkong.csv\")\n",
    "hungary=pd.read_csv(\"hungary.csv\")\n",
    "iceland=pd.read_csv(\"iceland.csv\")\n",
    "indonesia=pd.read_csv(\"indonesia.csv\")\n",
    "ireland=pd.read_csv(\"ireland.csv\")\n",
    "italy=pd.read_csv(\"italy.csv\")\n",
    "japan=pd.read_csv(\"japan.csv\")\n",
    "latvia=pd.read_csv(\"latvia.csv\")\n",
    "lithuania=pd.read_csv(\"lithuania.csv\")\n",
    "luxembourg=pd.read_csv(\"luxembourg.csv\")\n",
    "malaysia=pd.read_csv(\"malaysia.csv\")\n",
    "mexico=pd.read_csv(\"mexico.csv\")\n",
    "netherlands=pd.read_csv(\"netherlands.csv\")\n",
    "newzealand=pd.read_csv(\"newzealand.csv\")\n",
    "norway=pd.read_csv(\"norway.csv\")\n",
    "panama=pd.read_csv(\"panama.csv\")\n",
    "paraguay=pd.read_csv(\"paraguay.csv\")\n",
    "peru=pd.read_csv(\"peru.csv\")\n",
    "philippines=pd.read_csv(\"philippines.csv\")\n",
    "poland=pd.read_csv(\"poland.csv\")\n",
    "portugal=pd.read_csv(\"portugal.csv\")\n",
    "singapore=pd.read_csv(\"singapore.csv\")\n",
    "slovakia=pd.read_csv(\"slovakia.csv\")\n",
    "spain=pd.read_csv(\"spain.csv\")\n",
    "sweden=pd.read_csv(\"sweden.csv\")\n",
    "switzerland=pd.read_csv(\"switzerland.csv\")\n",
    "taiwanprovinceofchina=pd.read_csv(\"taiwan.csv\")\n",
    "turkey=pd.read_csv(\"turkey.csv\")\n",
    "unitedkingdom=pd.read_csv(\"unitedkingdom.csv\")\n",
    "unitedstates=pd.read_csv(\"unitedstates.csv\")\n",
    "uruguay=pd.read_csv(\"uruguay.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_dic = {\"argentina\":argentina, \"australia\":australia, \"austria\": austria, \"belgium\": belgium,\n",
    "#                  \"bolivia\":bolivia, \"brazil\":brazil,\"canada\":canada, \"chile\": chile, \"colombia\":colombia,\n",
    "#                  \"costarica\":costarica, \"czechrepublic\":czechrepublic,\"denmark\":denmark, \n",
    "#                  \"dominicanrepublic\":dominicanrepublic,\"ecuador\":ecuador,\"elsalvador\":elsalvador,\n",
    "#                  \"estonia\":estonia, \"finland\":finland,\"france\":france,\"germany\":germany,\"Global\":Global,\n",
    "#                  \"greece\":greece,\"guatemala\":guatemala,\"honduras\":honduras,\"hongkong\":hongkong,\"hungary\":hungary,\n",
    "#                  \"iceland\":iceland,\"indonesia\":indonesia,\"ireland\":ireland,\"italy\":italy,\"japan\":japan,\n",
    "#                  \"latvia\":latvia,\"lithuania\":lithuania,\"luxembourg\":luxembourg,\"malaysia\":malaysia,\"mexico\":mexico,\n",
    "#                  \"netherlands\":netherlands, \"newzealand\":newzealand,\"norway\":norway,\"panama\":panama,\n",
    "#                  \"paraguay\":paraguay,\"peru\":peru,\"philippines\":philippines,\"poland\":poland,\"portugal\":portugal,\n",
    "#                  \"singapore\":singapore,\"slovakia\":slovakia,\"spain\":spain,\"sweden\":sweden,\"switzerland\":switzerland,\n",
    "#                  \"taiwanprovinceofchina\":taiwanprovinceofchina,\"turkey\":turkey,\"unitedkingdom\":unitedkingdom,\n",
    "#                  \"unitedstates\":unitedstates,\"uruguay\":uruguay}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creating a new happiness dataframe that contains only the relevant countries\n",
    "As noted in our dataset descriptions, the \"Worldwide Happiness Ranking\\\" (happy17) contains happiness data for 155 countries, while \"Spotify's Worldwide Song Ranking\" contains only 54 countries. <br><br>We needed to find the overlapping countries between these datasets to:\n",
    "1. Create a new happiness ranking excluding the countries not found in the song ranking dataset, subsetted in the dataframe **happy**\n",
    "    <br>\n",
    "2. Figure out which \"Spotify's Worldwide Song Rankings\" country .csv files we do not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all 54 countries from \"Spotify's Worldwide Songs\"\n",
    "allspotifycountries=[\"argentina\", \"australia\", \"austria\", \"belgium\", \"brazil\",\"bolivia\", \"canada\", \"chile\", \"colombia\", \"costarica\", \"czechrepublic\",\"denmark\", \"dominicanrepublic\", \"estonia\", \"elsalvador\", \"Global\",\"greece\", \"ecuador\", \"finland\", \"france\", \"germany\", \"guatemala\", \"honduras\", \"hongkong\", \"hungary\", \"iceland\", \"indonesia\", \"ireland\", \"italy\", \"japan\", \"latvia\", \"lithuania\", \"luxembourg\", \"malaysia\", \"mexico\", \"netherlands\", \"newzealand\", \"norway\", \"panama\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\", \"singapore\", \"slovakia\", \"spain\", \"sweden\", \"switzerland\", \"taiwanprovinceofchina\", \"turkey\", \"unitedkingdom\", \"unitedstates\", \"uruguay\"]\n",
    "\n",
    "#list for countries that are found in both datsets.\n",
    "allcountries=[]\n",
    "for row in range(len(happy2017)):\n",
    "    country=happy2017.loc[row,'country']\n",
    "    country=country.lower()\n",
    "    country=country.replace(\" \",\"\")\n",
    "    if country in allspotifycountries:\n",
    "        allcountries.append(country)      \n",
    "\n",
    "#new happiness ranking dataframe \"happy\"        \n",
    "happy=pd.DataFrame({'country':[],'happiness_rank':[],'happiness_score':[]})\n",
    "for row in range(len(happy2017)):\n",
    "    country=happy2017.loc[row,'country']\n",
    "    country=country.lower()\n",
    "    country=country.replace(\" \",\"\")\n",
    "    if country in allcountries:       \n",
    "        newrow={'country':happy2017.loc[row,'country'],'happiness_rank':happy2017.loc[row,'happiness_rank'],'happiness_score':happy2017.loc[row,'happiness_score']}\n",
    "        happy=happy.append(newrow, ignore_index=True)\n",
    "\n",
    "print(\"There are \"+ str(len(allcountries)) +\" countries total that we can use to address our research question.\")\n",
    "print(\"\\n\")\n",
    "print('These are the spotify datsets we should use: ' + str(allcountries))\n",
    "print(\"\\n\")\n",
    "print(\"This is the updated happiness ranking:\")\n",
    "print(happy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data cleaning for \"Spotify's Worldwide Song Rankings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the countries we will work with based on the subset below in allcountries. We will use this to subset the first 50 songs below\n",
    "countries_to_subset = [norway, denmark, iceland, switzerland, finland, \n",
    "                       netherlands, canada, newzealand, sweden, australia, costarica, austria, \n",
    "                       unitedstates, ireland, germany, belgium, luxembourg, unitedkingdom, \n",
    "                       chile, brazil, czechrepublic, argentina, mexico, singapore, uruguay, guatemala,\n",
    "                       panama, france, taiwanprovinceofchina, spain, colombia, slovakia, malaysia, ecuador, elsalvador, poland, \n",
    "                       italy, japan,lithuania, latvia, bolivia, peru, estonia, turkey, paraguay, \n",
    "                       philippines, hungary, indonesia, dominicanrepublic, greece, portugal, honduras]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Deleting indices that carried over from data.csv\n",
    "As seen in `norway` below, the first column for every country .csv contains an unnamed column. This column, `Unnamed: 0` contains the original indices from the data.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norway.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleancol(dataframe):\n",
    "    \"\"\"\n",
    "    This will get delete the unnamed column that contained the data.csv indices \n",
    "    \n",
    "    Parameter dataframe: this is the country's dataframe which we will work with.\n",
    "    Precondition: a pandas dataframe object\n",
    "    \"\"\"\n",
    "    dataframe.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "    dataframe.drop([\"a\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #running cleancol on every .csv\n",
    "# for country in countries_to_subset:\n",
    "#     cleancol(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each country .csv no longer contains the old indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norway.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. We don't need the top 200 songs per day in a year\n",
    "The \"Spotify Worldwide Song Rankings\" from Kaggle is far too excessive for our analysis and research purposes. Providing the top 200 songs per day in a year means that each country .csv file should have around (365+9)x200=74,800 entries (the additional 9 days are because the data set includes the first 9 days of 2018). We decided that we would remove the bottom 150 songs per day in each country's dataset. <br>\n",
    "It is possible that we may decide to remove even more even later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the countries we will work with based on the subset below in allcountries. We will use this to subset the first 50 songs below\n",
    "countries_to_subset = [norway, denmark, iceland, switzerland, finland, \n",
    "                       netherlands, canada, newzealand, sweden, australia, costarica, austria, \n",
    "                       unitedstates, ireland, germany, belgium, luxembourg, unitedkingdom, \n",
    "                       chile, brazil, czechrepublic, argentina, mexico, singapore, uruguay, guatemala,\n",
    "                       panama, france, taiwanprovinceofchina, spain, colombia, slovakia, malaysia, ecuador, elsalvador, poland, \n",
    "                       italy, japan,lithuania, latvia, bolivia, peru, estonia, turkey, paraguay, \n",
    "                       philippines, hungary, indonesia, dominicanrepublic, greece, portugal, honduras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#associates a number with the index\n",
    "#find what index matches with the country. use index()\n",
    "# for index, country in enumerate(countries_to_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_fifty(dataframe):\n",
    "    \"\"\"\n",
    "    This will get the first 50 observations in every 200 observations.\n",
    "    There should only be 200 observations in 1 day, and there are 365 days per country in this data,\n",
    "    which is the purpose of this function.\n",
    "    \n",
    "    Parameter dataframe: this is the country's dataframe which we will work with.\n",
    "    Precondition: a pandas dataframe object\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.groupby(\"Date\").head(50)\n",
    "    dataframe=dataframe.reset_index(drop=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILED CODE!!! Please see VI. QUESTIONS FOR REVIEWERS\n",
    "# Our for-loop did not work, as shown in our print statements\n",
    "\n",
    "#running each country in countries_to_subset through the first_fifty procedure\n",
    "print(\"Now deleting the bottom 150 songs per day in each country...\")\n",
    "x=0\n",
    "for file in countries_to_subset:\n",
    "    print(allcountries[x]+\" length before: \"+str(len(countries_to_subset[x])))\n",
    "#   first_fifty(file) #Did not work\n",
    "    file=first_fifty(file) #Did not work\n",
    "#   countries_to_subset[x]=first_fifty(file) #Did not work\n",
    "    print(allcountries[x]+\" length after: \"+str(len(countries_to_subset[x])))\n",
    "    x=x+1\n",
    "print(\"\\n\")\n",
    "print(\"Unfortunately the lengths are the same before and after. So we'll have to call first_fifty individally for now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # doing it by hand because the for-loop did not work\n",
    "# norway = first_fifty(norway)\n",
    "# denmark= first_fifty(denmark)\n",
    "# iceland = first_fifty(iceland)\n",
    "# switzerland= first_fifty(switzerland)\n",
    "# finland = first_fifty(finland)\n",
    "# netherlands= first_fifty(netherlands)\n",
    "# canada = first_fifty(canada)\n",
    "# newzealand= first_fifty(newzealand)\n",
    "# sweden= first_fifty(sweden)\n",
    "# australia= first_fifty(australia)\n",
    "# costarica= first_fifty(costarica)\n",
    "# austria = first_fifty(austria)\n",
    "# unitedstates= first_fifty(unitedstates)\n",
    "# ireland= first_fifty(ireland)\n",
    "# germany= first_fifty(germany)\n",
    "# belgium= first_fifty(belgium)\n",
    "# luxembourg= first_fifty(luxembourg)\n",
    "# unitedkingdom= first_fifty(unitedkingdom)\n",
    "# chile= first_fifty(chile)\n",
    "# brazil= first_fifty(brazil)\n",
    "# czechrepublic= first_fifty(czechrepublic)\n",
    "# argentina= first_fifty(argentina)\n",
    "# mexico= first_fifty(mexico)\n",
    "# singapore= first_fifty(singapore)\n",
    "# uruguay= first_fifty(uruguay)\n",
    "# guatemala= first_fifty(guatemala)\n",
    "# panama= first_fifty(panama)\n",
    "# france= first_fifty(france)\n",
    "# spain= first_fifty(spain)\n",
    "# colombia= first_fifty(colombia)\n",
    "# slovakia= first_fifty(slovakia)\n",
    "# malaysia= first_fifty(malaysia)\n",
    "# ecuador= first_fifty(ecuador)\n",
    "# elsalvador= first_fifty(elsalvador)\n",
    "# poland= first_fifty(poland)\n",
    "# italy= first_fifty(italy)\n",
    "# japan= first_fifty(japan)\n",
    "# lithuania= first_fifty(lithuania)\n",
    "# latvia= first_fifty(latvia)\n",
    "# bolivia= first_fifty(bolivia)\n",
    "# peru= first_fifty(peru)\n",
    "# estonia= first_fifty(estonia)\n",
    "# turkey= first_fifty(turkey)\n",
    "# paraguay= first_fifty(paraguay)\n",
    "# philippines= first_fifty(philippines)\n",
    "# hungary= first_fifty(hungary)\n",
    "# indonesia= first_fifty(indonesia)\n",
    "# dominicanrepublic= first_fifty(dominicanrepublic)\n",
    "# greece= first_fifty(greece)\n",
    "# portugal= first_fifty(portugal)\n",
    "# honduras= first_fifty(honduras)\n",
    "# taiwanprovinceofchina= first_fifty(taiwanprovinceofchina)\n",
    "# print(\"Now the entries for all dates in 2018 are gone!\")\n",
    "# norway.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. We cannot use the data from 2018\n",
    "We only have happiness rankings for the year of 2017, but the Spotify rankings start in January 1, 2017 and stop at January 9, 2018. Though this is only 9 days in 2018, we cannot use this part of the data set.\n",
    "<br>\n",
    "To maintain consistency in out datasets, the function below is excludes all observations, or songs, from 2018 accidentally subsetted in our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not18(dataframe):\n",
    "    dataframe['Date'] = pd.to_datetime(dataframe['Date']).copy()\n",
    "    dataframe = dataframe[dataframe['Date'].dt.year != 2018]\n",
    "    return dataframe\n",
    "#for each in country list, store the return in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILED CODE!!! See VI. Questions for Reviewers\n",
    "# We wanted not18 to run as a procedure so that we could loop it through countries_to_subset. Unfortunately this did not work, \n",
    "# so we had to call not18() on every country .csv\n",
    "\n",
    "# for country in countries_to_subset:\n",
    "#     country=cleancol(country)\n",
    "#     country=first_fifty(country)\n",
    "#     country=not18(country)\n",
    "\n",
    "\n",
    "# for i in range(len(countries_to_subset)):\n",
    "# #     countries_to_subset[i]=cleancol(countries_to_subset[i])\n",
    "#     countries_to_subset[i]=first_fifty(countries_to_subset[i])\n",
    "#     countries_to_subset[i]=not18(countries_to_subset[i])\n",
    "\n",
    "\n",
    "# Enumerate in a python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_to_subset[0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Descriptions\n",
    "- `happiness_score` the sum of each `happiness_score`, `economy_gdp_per_capita_`, `health_life_expectancy_`, `freedom`, `generosity`, `\ttrust_government_corruption_`, and `dystopia_residual` scores. These individual scores reflect the \"six factors\" used to calculate happiness in the description above.<br>\n",
    "- `Happiness.Rank` the ranking of each country's happiness scores, from highest happiness score to the lowest<br>\n",
    "- `Country` the country being ranked/scored<br>\n",
    "\n",
    "* **For our research purposes, we will only be keeping the following columns: `country`,`happiness_rank`, and `happiness_score`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. \"Spotify's World Song Rankings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norway.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Descriptions\n",
    "- `position` the rank of the song.<br>\n",
    "- `track_name` the of the song<br>\n",
    "- `artist` the artist<br>\n",
    "- `streams` the streams/day<br>\n",
    "- `url` the Spotify URL<br>\n",
    "- `date` the date the songs were streamed<br>\n",
    "- `region` country, by postal code<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Data Description\n",
    "1) What are the observations (rows) and the attributes (columns)? The observations are countries and the attributes are as stated above. \n",
    "\n",
    "2) Why was this dataset created? This dataset was originally created by the Sustainable Development Solutions Network and posted on Kaggle. The purpose of this dataset was to explore these questions:\n",
    "    - What countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness? \n",
    "    - How did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports? \n",
    "    - Did any country experience a significant increase or decrease in happiness?\n",
    "\n",
    "3) Who funded the creation of the dataset? The Sustainable Development Solutions Network is part of the UN, so we can assume that the creation of this dataset was funded by the United Nations.\n",
    "\n",
    "4) What processes might have influenced what data was observed and recorded and what was not? While Gallup has a very thorough methodology when it comes to carrying out the Gallup World Poll (which provides the data that is used to calculate the happiness scores and rankings), there are still chances that people are not honest when they answer the question, or that the poll is not reaching a large/diverse enough group. \n",
    "\n",
    "5) What preprocessing was done, and how did the data come to be in the form that you are using? The SDSN did a lot of preprocessing since we believe they are the ones who took the data from the Gallup World Poll and calculated all of the happiness data. \n",
    "\n",
    "6) If people are involved, were they aware of the data collection and if so, what purpose did they expect the data to be used for? The happiness scores are calculated using data that is collected through the Gallup World Poll. The people who answer these questions are aware of the data collection. This is a well-known report that is compiled, however, we are unsure if they are aware of the purpose of the survey.\n",
    "\n",
    "7) Where can your raw source data be found, if applicable? Provide a link to the raw data (hosted in a Cornell Google Drive or Cornell Box).  This is the link to the dataset that was uploaded to Kaggle by the SDSN: https://www.kaggle.com/unsdsn/world-happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Data Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Given we are dealing with a relatively sample of only 2017, albeit with 50 songs per day, drawing conclusions for what type of music a happy or unhappy country listens may not prove out to be as accurate as we would like it to be. We can say, however, that the culture of a country likely does not vary signficantly from one year to another, so the top genres and happy scores may reflect of the country's values to an extent.\n",
    "\n",
    "2) Some countries were omitted in order to be able to use the countries in the happiness index and those in our spotify data. In order to draw some observations, we had to find overlap in songs. This may naturally produce bias since some potentially happy countries with potentially signficant relationships to music will completely be overlooked due to availability of data. This concerns mainly the overarching/big picture of happy countries and certain genres being more common.\n",
    "\n",
    "3) We are relying on audio features scores for specific songs provided by a dataset. It is possible this dataset contains subjective information to the user, hence the genres and danceability scores may not reflect the names the people of the country would utilize.\n",
    "\n",
    "4) Certain countries use other streaming services opposed to Spotify. For example, Japan uses has it's own streaming service that is more popular than Spotify. Therefore, we techinically aren't getting the most popular songs of reach country, but rather the most popular songs offered on Spotify for each country.\n",
    "\n",
    "5) We removed over half of the countries on the Happiness Index. Therefore when we begin to categorize countries as \"sad\", they may not actually be very \"sad\" in comparison to the countries we removed. It will be important for us to look at the scores as well as the ranks.\n",
    "\n",
    "6) We are are comparing happiness from 2017, which is about 4 years ago. Trends in music have changed and we need to understand what is popular now is different from what it was back then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filez = [norway, denmark, iceland, switzerland, finland, \n",
    "                       netherlands, canada, newzealand, sweden, australia, costarica, austria, \n",
    "                       unitedstates, ireland, germany, belgium, luxembourg, unitedkingdom, \n",
    "                       chile, brazil, czechrepublic, argentina, mexico, singapore, uruguay, guatemala,\n",
    "                       panama, france, taiwanprovinceofchina, spain, colombia, slovakia, malaysia, ecuador, elsalvador, poland, \n",
    "                       italy, japan,lithuania, latvia, bolivia, peru, estonia, turkey, paraguay, \n",
    "                       philippines, hungary, indonesia, dominicanrepublic, greece, portugal, honduras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(d):\n",
    "    '''\n",
    "    changes column names to all lower case \n",
    "    '''\n",
    "    new_colnames = d.columns\n",
    "    new_colnames = [x.lower().replace(' ', '_') for x in new_colnames]\n",
    "#     d = d.copy()\n",
    "    d.columns=new_colnames\n",
    "\n",
    "    d.dropna(inplace=True)\n",
    "    d.reset_index(drop=True,inplace=True)\n",
    "    return d\n",
    "#     return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy2017columns= [x.lower() for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\"..\",\".\") for x in happy2017columns]\n",
    "happy2017columns= [x.replace(\".\",\"_\") for x in happy2017columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for file in filez:\n",
    "#     change(file)\n",
    "    \n",
    "    \n",
    "for i in range(len(countries_to_subset)):\n",
    "#     countries_to_subset[i]=cleancol(countries_to_subset[i])\n",
    "    countries_to_subset[i]=first_fifty(countries_to_subset[i])\n",
    "    countries_to_subset[i]=not18(countries_to_subset[i])\n",
    "    countries_to_subset[i]=change(countries_to_subset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_subset[i].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if our change() method successfully changed the column names and dropped rows that contained NAN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norway.shape)\n",
    "norway.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge separate country dataframes for ease of later analysis using concat\n",
    "main=pd.DataFrame()\n",
    "for country in countries_to_subset:\n",
    "    main=pd.concat([main,country])\n",
    "    main=main.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupcount=main.groupby('region').count()\n",
    "groupcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "\n",
    "plt.scatter(happy['country'], happy['happiness_score'])\n",
    "plt.xlabel('Country', fontsize=50)\n",
    "plt.ylabel('Happiness Score', fontsize=50)\n",
    "plt.title('Country Happiness Score 2017', fontsize=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if the spotipy sp.audiofeatures() method works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.audio_features('5aAx2yezTd8zXrkmtKl66Z')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is all of our previous attempts to write a method that we could use to call sp.audio_features() on the tracks in each country's dataframe. While it looks very messy, we are keeping it in case we can use any of it to find a more effective method than the one we came up with for this phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now creating a method that will enable us to call the sp.audiofeatures() method. After some trial and error, in which we attempted to call use the Spotify API method on every URL on every song, in every row of the dataframe of each country, we realized that our method could never finish executing because the dataframes were so large and also that the Spotify API does have a limit on how many times you call their methods (we weren't allowed to use the Spotify API methods for a certain amount of time).\n",
    "\n",
    "We developed getunique() to first find all of the unique songs in each dataframe. We realized that for the top daily songs for a year, there is obviously a lot of repititon of songs. So using pd.unique() enabled us to find a way to reduce the amount of times we needed to call sp.audio_features(). This method returns a dataframe that is a collection of the audio features information for every unique song in each country's dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the top 10 and bottom 10 happiest countries\n",
    "topbot=pd.DataFrame()\n",
    "topbot=pd.concat([topbot, happy.head(10)])\n",
    "topbot=pd.concat([topbot, happy.tail(10)])\n",
    "topbot=topbot.reset_index(drop=True)\n",
    "topbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main.groupby('region').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=main.region.unique()\n",
    "new=names[:10]\n",
    "new2=(names[len(names)-10:len(names)])\n",
    "finalnames=np.append(new,new2).tolist()\n",
    "\n",
    "finalsongs=main.loc[main['region'].isin(finalnames)]\n",
    "finalsongs=finalsongs.reset_index(drop=True)\n",
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs['country']=finalsongs['region']\n",
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict={'no':\"Norway\", 'dk':\"Denmark\", 'ch':\"Switzerland\", 'fi':\"Finland\", 'nl':\"Netherlands\", 'se':\"Sweden\", 'ee':\"Estonia\", 'hu':\"Hungary\", 'id':\"Indonesia\", 'is':\"Iceland\", 'ca':\"Canada\",\n",
    "       'nz':\"New Zealand\", 'au':\"Australia\", 'tr':\"Turkey\", 'py':\"Paraguay\", 'ph':\"Philippines\", 'do':\"Dominican Republic\", 'gy':\"Greece\", 'pt':\"Portugal\", 'hn':\"Honduras\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs.country=finalsongs['country'].map(region_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify=topbot.merge(finalsongs, on=\"country\")\n",
    "spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniquefinal=finalsongs.copy()\n",
    "# uniquefinal=pd.unique(uniquefinal['track_name'])\n",
    "# uniquefinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify.groupby('region').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spotify(dataframe):\n",
    "#     audiofeatures=pd.DataFrame({'danceability':[], 'energy':[],'key':[], 'loudness':[], 'mode':[], 'speechiness':[],'acousticness':[], 'instrumentalness':[],'liveness':[], 'valence':[], 'tempo':[], 'type':[],'id':[], 'uri':[],'track_href':[], 'analysis_url':[], 'duration_ms':[], 'time_signature':[]})\n",
    "#     for row in range(len(dataframe)):\n",
    "#         url=dataframe.loc[row,'url']\n",
    "#         sub=url.rindex('/')\n",
    "#         idurl=url[sub+1:]\n",
    "#         newrow=sp.audio_features(idurl)[0]\n",
    "#         audiofeatures=audiofeatures.append(newrow,ignore_index=True)\n",
    "\n",
    "#     return audiofeatures       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniquefinal.drop_duplicates(subset =\"track_name\",\n",
    "#                      keep = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getunique(dataframe):\n",
    "    audiofeatures=pd.DataFrame({'danceability':[], 'energy':[],'key':[], 'loudness':[], 'mode':[], 'speechiness':[],'acousticness':[], 'instrumentalness':[],'liveness':[], 'valence':[], 'tempo':[], 'type':[],'id':[], 'uri':[],'track_href':[], 'analysis_url':[], 'duration_ms':[], 'time_signature':[]})\n",
    "    dataframe.drop_duplicates(subset =\"track_name\",\n",
    "                     keep = 'first', inplace = True,)\n",
    "    dataframe=dataframe.reset_index(drop=True)\n",
    "#     dataframe['track_name']\n",
    "#     uniquesongs=pd.unique(dataframe['track_name'])\n",
    "#     list=[]\n",
    "        \n",
    "    for row in range(len(dataframe)):\n",
    "#          for song in uniquesongs:\n",
    "#             if dataframe.loc[row,'track_name']==song and not song in list:\n",
    "            url=dataframe.loc[row,'url']\n",
    "            sub=url.rindex('/')\n",
    "            idurl=url[sub+1:]\n",
    "            newrow=sp.audio_features(idurl)[0]\n",
    "            audiofeatures=audiofeatures.append(newrow,ignore_index=True)\n",
    "#             list.append(song)\n",
    "    audiofeatures['track_name']=dataframe['track_name']\n",
    "    audiofeatures['happiness_score']=dataframe['happiness_score']\n",
    "    audiofeatures['happiness_rank']=dataframe['happiness_rank']\n",
    "    audiofeatures['country']=dataframe['country']\n",
    "    return audiofeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify=getunique(spotify.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=uniquespotify.copy()\n",
    "uniquespotify=uniquespotify.drop(['analysis_url','time_signature','track_href','uri','type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify=uniquespotify[['country', 'happiness_score', 'happiness_rank','track_name', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "       'id', 'duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquespotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save2=uniquespotify.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniquespotify[uniquespotify['track_name'] == 'Cásate Conmigo'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated=pd.unique(uniquespotify['track_name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep2=[]\n",
    "# for song in repeated: \n",
    "#     rep2.append(finalsongs[finalsongs[\"track_name\"]==song].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finalsongs['danceability']=np.nan\n",
    "# finalsongs['energy']=np.nan\n",
    "# finalsongs['key']=np.nan\n",
    "# finalsongs['loudness']=np.nan\n",
    "# finalsongs['mode']=np.nan\n",
    "# finalsongs['speechiness']=np.nan\n",
    "# finalsongs['acousticness']=np.nan\n",
    "# finalsongs['instrumentalness']=np.nan\n",
    "# finalsongs['liveness']=np.nan\n",
    "# finalsongs['valence']=np.nan\n",
    "# finalsongs['tempo']=np.nan\n",
    "# finalsongs['id']=np.nan\n",
    "# finalsongs['duration_ms']=np.nan\n",
    "# finalsongs['happiness_score']=np.nan\n",
    "# finalsongs['happiness_rank']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(uniquespotify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rownum=uniquespotify[uniquespotify['track_name'] == 'Alone'].index[0]\n",
    "# rownum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_add=pd.DataFrame()\n",
    "# to_add['danceability']=np.nan\n",
    "# to_add['energy']=np.nan\n",
    "# to_add['key']=np.nan\n",
    "# to_add['loudness']=np.nan\n",
    "# to_add['mode']=np.nan\n",
    "# to_add['speechiness']=np.nan\n",
    "# to_add['acousticness']=np.nan\n",
    "# to_add['instrumentalness']=np.nan\n",
    "# to_add['liveness']=np.nan\n",
    "# to_add['valence']=np.nan\n",
    "# to_add['tempo']=np.nan\n",
    "# to_add['id']=np.nan\n",
    "# to_add['duration_ms']=np.nan\n",
    "# to_add['happiness_score']=np.nan\n",
    "# to_add['happiness_rank']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to_add=pd.Dataframe()\n",
    "# to_add['danceability']=np.nan\n",
    "# # to_add['energy']=np.nan\n",
    "# # to_add['key']=np.nan\n",
    "# # to_add['loudness']=np.nan\n",
    "# # to_add['mode']=np.nan\n",
    "# # to_add['speechiness']=np.nan\n",
    "# # to_add['acousticness']=np.nan\n",
    "# # to_add['instrumentalness']=np.nan\n",
    "# # to_add['liveness']=np.nan\n",
    "# # to_add['valence']=np.nan\n",
    "# # to_add['tempo']=np.nan\n",
    "# # to_add['id']=np.nan\n",
    "# # to_add['duration_ms']=np.nan\n",
    "# # to_add['happiness_score']=np.nan\n",
    "# # to_add['happiness_rank']=np.nan\n",
    "\n",
    "# count=-1\n",
    "# rownum=0\n",
    "# for item in range(len(rep2)):\n",
    "#     print(item)\n",
    "#     count=count+1\n",
    "#     for row in range(len(rep2[item])):\n",
    "#         dance=uniquespotify.loc[count,'danceability']\n",
    "#         energy=uniquespotify.loc[count,'energy']\n",
    "#         key=uniquespotify.loc[count,'key']\n",
    "#         loudness=uniquespotify.loc[count,'loudness']\n",
    "#         mode=uniquespotify.loc[count,'mode']\n",
    "#         speech=uniquespotify.loc[count,'speechiness']\n",
    "#         acoustic=uniquespotify.loc[count,'acousticness']\n",
    "#         instr=uniquespotify.loc[count,'instrumentalness']\n",
    "#         live=uniquespotify.loc[count,'liveness']\n",
    "#         val=uniquespotify.loc[count,'valence']\n",
    "#         temp=uniquespotify.loc[count,'tempo']\n",
    "#         ids=uniquespotify.loc[count,'id']\n",
    "#         dur=uniquespotify.loc[count,'duration_ms']\n",
    "#         track=uniquespotify.loc[count,'track_name']\n",
    "#         score=uniquespotify.loc[count,'happiness_score']\n",
    "#         rank=uniquespotify.loc[count,'happiness_rank']\n",
    "        \n",
    "        \n",
    "# #         to_add.loc[rep2[item][row],'danceability']=dance\n",
    "# #         to_add.loc[rep2[item][row],'energy']=energy\n",
    "# #         to_add.loc[rep2[item][row],'key']=key\n",
    "# #         to_add.loc[rep2[item][row],'loudness']=loudness\n",
    "# #         to_add.loc[rep2[item][row],'mode']=mode\n",
    "# #         to_add.loc[rep2[item][row],'speech']=speech\n",
    "# #         to_add.loc[rep2[item][row],'acoustic']=acoustic\n",
    "# #         to_add.loc[rep2[item][row],'instrumentalness']=instr\n",
    "# #         to_add.loc[rep2[item][row],'liveness']=live\n",
    "# #         to_add.loc[rep2[item][row],'valence']=val\n",
    "# #         to_add.loc[rep2[item][row],'tempo']=temp\n",
    "# #         to_add.loc[rep2[item][row],'id']=ids\n",
    "# #         to_add.loc[rep2[item][row],'duration_ms']=dur\n",
    "# #         to_add.loc[rep2[item][row],'track_name']=track\n",
    "# #         to_add.loc[rep2[item][row],'happiness_score']=score\n",
    "# #         to_add.loc[rep2[item][row],'happiness_rank']=rank\n",
    "\n",
    "#         finalsongs.loc[rep2[item][row],'danceability']=dance\n",
    "#         finalsongs.loc[rep2[item][row],'energy']=energy\n",
    "#         finalsongs.loc[rep2[item][row],'key']=key\n",
    "#         finalsongs.loc[rep2[item][row],'loudness']=loudness\n",
    "#         finalsongs.loc[rep2[item][row],'mode']=mode\n",
    "#         finalsongs.loc[rep2[item][row],'speech']=speech\n",
    "#         finalsongs.loc[rep2[item][row],'acoustic']=acoustic\n",
    "#         finalsongs.loc[rep2[item][row],'instrumentalness']=instr\n",
    "#         finalsongs.loc[rep2[item][row],'liveness']=live\n",
    "#         finalsongs.loc[rep2[item][row],'valence']=val\n",
    "#         finalsongs.loc[rep2[item][row],'tempo']=temp\n",
    "#         finalsongs.loc[rep2[item][row],'id']=ids\n",
    "#         finalsongs.loc[rep2[item][row],'duration_ms']=dur\n",
    "#         finalsongs.loc[rep2[item][row],'track_name']=track\n",
    "#         finalsongs.loc[rep2[item][row],'happiness_score']=score\n",
    "#         finalsongs.loc[rep2[item][row],'happiness_rank']=rank\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # to_add=pd.DataFrame({'danceability':[], 'energy':[],'key':[], 'loudness':[], 'mode':[], 'speechiness':[],'acousticness':[], 'instrumentalness':[],'liveness':[], 'valence':[], 'tempo':[], 'id':[],  'duration_ms':[], 'track_name':[], 'happiness_score':[],'happiness_rank':[]})\n",
    "\n",
    "# for row in range(len(finalsongs)):\n",
    "#     song=finalsongs.loc[row,'track_name']\n",
    "#     print(row)\n",
    "#     rownum=uniquespotify[uniquespotify[\"track_name\"] == song].index[0]\n",
    "    \n",
    "#     dance=uniquespotify.loc[rownum,'danceability']\n",
    "#     energy=uniquespotify.loc[rownum,'energy']\n",
    "#     key=uniquespotify.loc[rownum,'key']\n",
    "#     loudness=uniquespotify.loc[rownum,'loudness']\n",
    "#     mode=uniquespotify.loc[rownum,'mode']\n",
    "#     speech=uniquespotify.loc[rownum,'speechiness']\n",
    "#     acoustic=uniquespotify.loc[rownum,'acousticness']\n",
    "#     instr=uniquespotify.loc[rownum,'instrumentalness']\n",
    "#     live=uniquespotify.loc[rownum,'liveness']\n",
    "#     val=uniquespotify.loc[rownum,'valence']\n",
    "#     temp=uniquespotify.loc[rownum,'tempo']\n",
    "#     ids=uniquespotify.loc[rownum,'id']\n",
    "#     dur=uniquespotify.loc[rownum,'duration_ms']\n",
    "#     track=uniquespotify.loc[rownum,'track_name']\n",
    "#     score=uniquespotify.loc[rownum,'happiness_score']\n",
    "#     rank=uniquespotify.loc[rownum,'happiness_rank']\n",
    "    \n",
    "#     finalsongs.loc[row,'danceability']=dance\n",
    "#     finalsongs.loc[row,'energy']=energy\n",
    "#     finalsongs.loc[row,'key']=key\n",
    "#     finalsongs.loc[row,'loudness']=loudness\n",
    "#     finalsongs.loc[row,'mode']=mode\n",
    "#     finalsongs.loc[row,'speech']=speech\n",
    "#     finalsongs.loc[row,'acoustic']=acoustic\n",
    "#     finalsongs.loc[row,'instrumentalness']=instr\n",
    "#     finalsongs.loc[row,'liveness']=live\n",
    "#     finalsongs.loc[row,'valence']=val\n",
    "#     finalsongs.loc[row,'tempo']=temp\n",
    "#     finalsongs.loc[row,'id']=ids\n",
    "#     finalsongs.loc[row,'duration_ms']=dur\n",
    "#     finalsongs.loc[row,'track_name']=track\n",
    "#     finalsongs.loc[row,'happiness_score']=score\n",
    "#     finalsongs.loc[row,'happiness_rank']=rank\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #     newrow={'danceability':dance, 'energy':energy,'key':key, 'loudness':loudness, 'mode':mode, 'speechiness':speech,'acousticness':acoustic, 'instrumentalness':instr,'liveness':live, 'valence':val, 'tempo':temp, 'id':ids,  'duration_ms':dur, 'track_name':track, 'happiness_score':score,'happiness_rank':rank}\n",
    " \n",
    "# #     to_add.append(newrow,ignore_index=True)\n",
    "\n",
    "# # to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs=finalsongs.merge(uniquespotify[['danceability','track_name' ,'happiness_score', 'happiness_rank', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'id', 'duration_ms']], how=\"left\",on='track_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs.rename({\"unnamed:_0\":\"a\"}, axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs.drop('a',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs=finalsongs[['country','happiness_score', 'happiness_rank','date','position', 'track_name', 'artist', 'streams', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness','instrumentalness', 'liveness', 'valence', 'tempo', 'id','duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling=pd.DataFrame()\n",
    "rolling = finalsongs.groupby('country', sort=False)[['danceability']].rolling(window=14).mean().reset_index()\n",
    "# rolling['avg_danceability']=finalsongs['danceability'].rolling(14).mean()\n",
    "rolling=rolling.merge(topbot[['country','happiness_score']], how='left',on='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter=sns.relplot(x='danceability', y='happiness_score', hue='country', data=rolling)\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.tight_layout\n",
    "# x_ticks = np.arange(0, .01, 8)\n",
    "# plt.xticks(x_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if the getunique() method worked and using .describe() to get same basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are looping through all of the country dataframes, calling .describe() on them (to make retrieving information about the audio feature averages a little easier). We will then add this information back into the happy dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newcolhappydance=[]\n",
    "# newcolhappyenergy=[]\n",
    "# newcolhappykey=[]\n",
    "# newcolhappyloudness=[]\n",
    "# newcolhappymode=[]\n",
    "# newcolhappyspeechiness=[]\n",
    "# newcolhappyacousticness=[]\n",
    "# newcolhappyinstrumentalness=[]\n",
    "# newcolhappyliveness=[]\n",
    "# newcolhappyvalence=[]\n",
    "# newcolhappytempo=[]\n",
    "# for file in affilez:\n",
    "#     meandance=file.describe().loc['mean','danceability']\n",
    "#     meanenergy=file.describe().loc['mean','energy']\n",
    "#     meankey=file.describe().loc['mean','key']\n",
    "#     meanloud=file.describe().loc['mean','loudness']\n",
    "#     meanmode=file.describe().loc['mean','mode']\n",
    "#     meanspeech=file.describe().loc['mean','speechiness']\n",
    "#     meanacoustic=file.describe().loc['mean','acousticness']\n",
    "#     meaninstr=file.describe().loc['mean','instrumentalness']\n",
    "#     meanlive=file.describe().loc['mean','liveness']\n",
    "#     meanval=file.describe().loc['mean','valence']\n",
    "#     meantemp=file.describe().loc['mean','tempo']\n",
    "#     newcolhappydance.append(meandance)\n",
    "#     newcolhappyenergy.append(meanenergy)\n",
    "#     newcolhappykey.append(meankey)\n",
    "#     newcolhappyloudness.append(meanloud) \n",
    "#     newcolhappymode.append(meanmode) \n",
    "#     newcolhappyspeechiness.append(meanspeech)\n",
    "#     newcolhappyacousticness.append(meanacoustic)\n",
    "#     newcolhappyinstrumentalness.append(meaninstr)\n",
    "#     newcolhappyliveness.append(meanlive)\n",
    "#     newcolhappyvalence.append(meanval)\n",
    "#     newcolhappytempo.append(meantemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy['avg_danceability'],happy['avg_energy'],happy['avg_key'],happy['avg_loudness'],happy['avg_mode'],happy['avg_speechiness'],happy['avg_acousticness'], happy['avg_instrumentalness'], happy['avg_liveness'], happy['avg_valence'], happy['avg_tempo']= [newcolhappydance, newcolhappyenergy, newcolhappykey, newcolhappyloudness, newcolhappymode, newcolhappyspeechiness, newcolhappyacousticness, newcolhappyinstrumentalness, newcolhappyliveness, newcolhappyvalence, newcolhappytempo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are printing the original happy dataframe but with the average spotify audio feature score for their top daily songs in its own column. This will be useful for any computations or graphic we try and make later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(80,20))\n",
    "# ax.plot(happy['country'], happy['happiness_score'], color=\"red\", marker=\"o\")\n",
    "# ax.set_xlabel(\"country\",fontsize=50)\n",
    "# ax.set_ylabel(\"Happiness Score\",color=\"red\",fontsize=50)\n",
    "# ax2=ax.twinx()\n",
    "# ax2.plot(happy['country'], happy['avg_danceability'] ,color=\"blue\",marker=\"o\")\n",
    "# ax2.set_ylabel(\"Average Danceability\",color=\"blue\",fontsize=50)\n",
    "# plt.title('Happiness Score and Danceability of Top Songs', fontsize=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(80,20))\n",
    "# ax.plot(happy['country'], happy['happiness_score'], color=\"red\", marker=\"o\")\n",
    "# ax.set_xlabel(\"country\",fontsize=50)\n",
    "# ax.set_ylabel(\"Happiness Score\",color=\"red\",fontsize=50)\n",
    "# ax2=ax.twinx()\n",
    "# ax2.plot(happy['country'], happy['avg_energy'] ,color=\"green\",marker=\"o\")\n",
    "# ax2.set_ylabel(\"Average Energy\",color=\"green\",fontsize=50)\n",
    "# plt.title('Happiness Score and Energy of Top Songs', fontsize=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(80,20))\n",
    "# ax.plot(happy['country'], happy['happiness_score'], color=\"red\", marker=\"o\")\n",
    "# ax.set_xlabel(\"country\",fontsize=50)\n",
    "# ax.set_ylabel(\"Happiness Score\",color=\"red\",fontsize=50)\n",
    "# ax2=ax.twinx()\n",
    "# ax2.plot(happy['country'], happy['avg_loudness'] ,color=\"black\",marker=\"o\")\n",
    "# ax2.set_ylabel(\"Average Loudness\",color=\"black\",fontsize=50)\n",
    "# plt.title('Happiness Score and Loudness of Top Songs', fontsize=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(80,20))\n",
    "# ax.plot(happy['country'], happy['happiness_score'], color=\"red\", marker=\"o\")\n",
    "# ax.set_xlabel(\"country\",fontsize=50)\n",
    "# ax.set_ylabel(\"Happiness Score\",color=\"red\",fontsize=50)\n",
    "# ax2=ax.twinx()\n",
    "# ax2.plot(happy['country'], happy['avg_key'] ,color=\"pink\",marker=\"o\")\n",
    "# ax2.set_ylabel(\"Average Key\",color=\"pink\",fontsize=50)\n",
    "# plt.title('Happiness Score and Key of Top Songs', fontsize=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at these graphs, there does not seem to be any clear relationship between the song features and the happinness score of a country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Questions for reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We think that we are using the spotify API correctly, however we can’t get it to work in the for-loop and went to multiple office hours trying to resolve this issue. Any suggestions would be appreciated! We decided to implement getunique() because we thought that this would make the program more efficient, but we still aren't sure if this is the best approach.\n",
    "\n",
    "2. When comparing country to country, we're pretty concerned about sampling and random sampling, social events can happen on certain days and affect moods for different countries. How should we go about comparing days in countries?\n",
    "\n",
    "3. For loop issues in data cleaning (procedures we could call on dataframes in a for loop instead of calling it individually on every country). We're not really sure why it's not working\n",
    "\n",
    "4. What graphs/visualizations would you recommend for us to use?\n",
    "\n",
    "5. In the first_fifty method, you can see that not all countries have the same length of songs. How would we then be able to accurately get the top 50 of every song?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
